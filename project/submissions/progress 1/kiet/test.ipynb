{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql.functions import col\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Anime Recommendation System\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the anime dataset\n",
    "def load_anime_data(filepath):\n",
    "    df = spark.read.csv(filepath, header=True, inferSchema=True)\n",
    "    \n",
    "    ratings_df = df.select(\"user_id\", \"anime_id\", \"rating\")\n",
    "    \n",
    "    ratings_df = ratings_df.withColumn(\"rating\", col(\"rating\").cast(\"float\"))\n",
    "    \n",
    "    return df, ratings_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(ratings_df, train_ratio=0.8):\n",
    "    return ratings_df.randomSplit([train_ratio, 1 - train_ratio], seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create & train ALS Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decided to use ALS model due to these given reasons:\n",
    "\n",
    "ALS is a matrix factorization algorithm designed for large-scale recommendation systems. It’s particularly useful when:\n",
    "\n",
    "- The dataset is sparse (i.e., most users have rated only a small fraction of all available items).\n",
    "- There are implicit or explicit ratings (ALS can handle both).\n",
    "- Scalability is a concern (Spark’s ALS is optimized for distributed computing).\n",
    "- Given that the Anime Dataset (2023) consists of user ratings for anime titles, ALS is a strong choice because:\n",
    "\n",
    "1. It can generalize well to unseen users and items by learning latent factors.\n",
    "2. It works well with sparse data, which is common in recommendation problems.\n",
    "3. It’s optimized for large datasets, making it a good fit for Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to other methods:\n",
    "- User-based or item-based collaborative filtering (kNN-based methods) don’t scale well for large datasets.\n",
    "- Content-based filtering doesn’t generalize well if metadata is missing or inconsistent.\n",
    "- ALS balances scalability and predictive performance better than most traditional models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_als_model(train_df, max_iter=10, reg_param=0.1, rank=10):\n",
    "    als = ALS(\n",
    "        maxIter=max_iter,\n",
    "        regParam=reg_param,\n",
    "        rank=rank,\n",
    "        userCol=\"user_id\",\n",
    "        itemCol=\"anime_id\",\n",
    "        ratingCol=\"rating\",\n",
    "        coldStartStrategy=\"drop\",  # Handle missing values by dropping them during evaluation\n",
    "        nonnegative=True  # Constrain the factors to be non-negative\n",
    "    )\n",
    "    \n",
    "    model = als.fit(train_df)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_df):\n",
    "    predictions = model.transform(test_df)\n",
    "    \n",
    "    # Drop NaN values that might have been introduced\n",
    "    predictions = predictions.na.drop()\n",
    "    \n",
    "    # Evaluate using RMSE\n",
    "    evaluator = RegressionEvaluator(\n",
    "        metricName=\"rmse\",\n",
    "        labelCol=\"rating\",\n",
    "        predictionCol=\"prediction\"\n",
    "    )\n",
    "    rmse = evaluator.evaluate(predictions)\n",
    "    \n",
    "    return rmse, predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate recommendations for a user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_recommendations(model, user_id, num_recommendations=10):\n",
    "    # Get top N recommendations for the user\n",
    "    user_recs = model.recommendForUserSubset(\n",
    "        spark.createDataFrame([(user_id,)], [\"user_id\"]),\n",
    "        num_recommendations\n",
    "    )\n",
    "    \n",
    "    return user_recs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate recommendations for an anime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_anime_recommendations(model, anime_id, num_recommendations=10):\n",
    "    # Get top N users\n",
    "    anime_recs = model.recommendForItemSubset(\n",
    "        spark.createDataFrame([(anime_id,)], [\"anime_id\"]),\n",
    "        num_recommendations\n",
    "    )\n",
    "    \n",
    "    return anime_recs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main function to run the recommendation system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function to run the recommendation system\n",
    "def run_anime_recommendation_system(filepath, user_id_to_recommend=None):\n",
    "    print(\"Loading data...\")\n",
    "    full_df, ratings_df = load_anime_data(filepath)\n",
    "    \n",
    "    print(f\"Total ratings: {ratings_df.count()}\")\n",
    "    print(f\"Unique users: {ratings_df.select('user_id').distinct().count()}\")\n",
    "    print(f\"Unique anime: {ratings_df.select('anime_id').distinct().count()}\")\n",
    "    \n",
    "    print(\"Splitting data into training and test sets...\")\n",
    "    train_df, test_df = split_data(ratings_df)\n",
    "    \n",
    "    print(\"Training ALS model...\")\n",
    "    model = train_als_model(train_df)\n",
    "    \n",
    "    print(\"Evaluating model...\")\n",
    "    rmse, predictions = evaluate_model(model, test_df)\n",
    "    print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n",
    "    \n",
    "    if user_id_to_recommend:\n",
    "        print(f\"Generating recommendations for user {user_id_to_recommend}...\")\n",
    "        user_recs = get_user_recommendations(model, user_id_to_recommend)\n",
    "        \n",
    "        user_recs_pd = user_recs.toPandas()\n",
    "        \n",
    "        # Join with anime names\n",
    "        if 'anime_name' in full_df.columns:\n",
    "            anime_names = full_df.select(\"anime_id\", \"anime_name\").distinct()\n",
    "            \n",
    "            # Extract recommendations from the nested structure\n",
    "            from pyspark.sql.functions import explode\n",
    "            user_recs_exploded = user_recs.select(\n",
    "                \"user_id\", \n",
    "                explode(\"recommendations\").alias(\"rec\")\n",
    "            )\n",
    "            user_recs_flattened = user_recs_exploded.select(\n",
    "                \"user_id\", \n",
    "                col(\"rec.anime_id\").alias(\"anime_id\"), \n",
    "                col(\"rec.rating\").alias(\"predicted_rating\")\n",
    "            )\n",
    "            \n",
    "            # Join with anime names\n",
    "            user_recs_with_names = user_recs_flattened.join(\n",
    "                anime_names, \n",
    "                on=\"anime_id\"\n",
    "            ).orderBy(col(\"predicted_rating\").desc())\n",
    "            \n",
    "            print(\"Top recommended anime:\")\n",
    "            user_recs_with_names.show(10, truncate=False)\n",
    "    \n",
    "    return model, full_df, ratings_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example use case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Placeholder for path\n",
    "    filepath = \"anime_ratings_2023.csv\"\n",
    "    \n",
    "    model, full_df, ratings_df = run_anime_recommendation_system(filepath, user_id_to_recommend=123)\n",
    "    \n",
    "    # Optional: Save the model\n",
    "    model.save(\"anime_als_model\")\n",
    "    \n",
    "    spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Completed tasks:\n",
    "- Setting up the recommendation system models\n",
    "- Read datasets and train/test split \n",
    "- Explore the reason to use the ALS model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remaining tasks:\n",
    "- Integrating Spark with Redis database\n",
    "- Solve the problem with real-time data on Redis while using ALS with micro-batch processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
