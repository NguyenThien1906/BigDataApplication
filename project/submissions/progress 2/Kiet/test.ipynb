{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql.functions import col\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Anime Recommendation System\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import MinMaxScaler, StringIndexer\n",
    "from pyspark.sql.functions import col, min, max\n",
    "\n",
    "# Scaling our \"rating\" column\n",
    "scaler = MinMaxScaler(inputCol=\"rating\", outputCol=\"scaled_score\")\n",
    "scaler_model = scaler.fit(df)\n",
    "df = scaler_model.transform(df)\n",
    "\n",
    "# Encoding categorical data\n",
    "## Encoding user IDs\n",
    "user_indexer = StringIndexer(inputCol=\"user_id\", outputCol=\"user_encoded\")\n",
    "df = user_indexer.fit(df).transform(df)\n",
    "num_users = df.select(\"user_id\").distinct().count()\n",
    "\n",
    "## Encoding anime IDs\n",
    "anime_indexer = StringIndexer(inputCol=\"anime_id\", outputCol=\"anime_encoded\")\n",
    "df = anime_indexer.fit(df).transform(df)\n",
    "num_animes = df.select(\"anime_id\").distinct().count()\n",
    "\n",
    "# Printing dataset information\n",
    "min_rating = df.select(min(\"rating\")).collect()[0][0]\n",
    "max_rating = df.select(max(\"rating\")).collect()[0][0]\n",
    "print(\"Number of unique users: {}, Number of unique anime: {}\".format(num_users, num_animes))\n",
    "print(\"Minimum rating: {}, Maximum rating: {}\".format(min_rating, max_rating))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the anime dataset\n",
    "def load_anime_data(filepath):\n",
    "    df = spark.read.csv(filepath, header=True, inferSchema=True)\n",
    "    \n",
    "    ratings_df = df.select(\"user_id\", \"anime_id\", \"rating\")\n",
    "    \n",
    "    ratings_df = ratings_df.withColumn(\"rating\", col(\"rating\").cast(\"float\"))\n",
    "    \n",
    "    return df, ratings_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(ratings_df, train_ratio=0.8):\n",
    "    return ratings_df.randomSplit([train_ratio, 1 - train_ratio], seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create & train ALS Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decided to use ALS model due to these given reasons:\n",
    "\n",
    "ALS is a matrix factorization algorithm designed for large-scale recommendation systems. It’s particularly useful when:\n",
    "\n",
    "- The dataset is sparse (i.e., most users have rated only a small fraction of all available items).\n",
    "- There are implicit or explicit ratings (ALS can handle both).\n",
    "- Scalability is a concern (Spark’s ALS is optimized for distributed computing).\n",
    "- Given that the Anime Dataset (2023) consists of user ratings for anime titles, ALS is a strong choice because:\n",
    "\n",
    "1. It can generalize well to unseen users and items by learning latent factors.\n",
    "2. It works well with sparse data, which is common in recommendation problems.\n",
    "3. It’s optimized for large datasets, making it a good fit for Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to other methods:\n",
    "- User-based or item-based collaborative filtering (kNN-based methods) don’t scale well for large datasets.\n",
    "- Content-based filtering doesn’t generalize well if metadata is missing or inconsistent.\n",
    "- ALS balances scalability and predictive performance better than most traditional models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_als_model(train_df, max_iter=10, reg_param=0.1, rank=10):\n",
    "    als = ALS(\n",
    "        maxIter=max_iter,\n",
    "        regParam=reg_param,\n",
    "        rank=rank,\n",
    "        userCol=\"user_id\",\n",
    "        itemCol=\"anime_id\",\n",
    "        ratingCol=\"rating\",\n",
    "        coldStartStrategy=\"drop\",  # Handle missing values by dropping them during evaluation\n",
    "        nonnegative=True  # Constrain the factors to be non-negative\n",
    "    )\n",
    "    \n",
    "    model = als.fit(train_df)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_df):\n",
    "    predictions = model.transform(test_df)\n",
    "    \n",
    "    # Drop NaN values that might have been introduced\n",
    "    predictions = predictions.na.drop()\n",
    "    \n",
    "    # Evaluate using RMSE\n",
    "    evaluator = RegressionEvaluator(\n",
    "        metricName=\"rmse\",\n",
    "        labelCol=\"rating\",\n",
    "        predictionCol=\"prediction\"\n",
    "    )\n",
    "    rmse = evaluator.evaluate(predictions)\n",
    "    \n",
    "    return rmse, predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate recommendations for a user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read anime-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize SparkSession if you haven't already\n",
    "spark = SparkSession.builder.appName(\"AnimeRecommendation\").getOrCreate()\n",
    "\n",
    "spark_df = spark.read.option(\"header\", \"true\") \\\n",
    "                           .option(\"inferSchema\", \"true\") \\\n",
    "                           .csv(\"/kaggle/input/myanimelist-dataset/anime-dataset-2023.csv\")\n",
    "\n",
    "# Read the CSV file into a Spark DataFrame\n",
    "spark_df_anime = spark.read.option(\"header\", \"true\") \\\n",
    "                           .option(\"inferSchema\", \"true\") \\\n",
    "                           .csv(\"/input/myanimelist-dataset/anime-dataset-2023.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want my model to recommend only those animes that have been rated by at least a certain number of users. This threshold helps ensure that the recommended anime titles have received a sufficient number of ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "popularity_threshold = 50\n",
    "spark_df_anime = spark_df_anime.filter(spark_df_anime[\"Members\"] >= popularity_threshold)\n",
    "\n",
    "row_count = spark_df_anime.count()\n",
    "column_count = len(spark_df_anime.columns)\n",
    "print(f\"DataFrame shape: ({row_count}, {column_count})\")\n",
    "\n",
    "spark_df_anime.show(3, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_recommendations(model, user_id, num_recommendations=10):\n",
    "    # Get top N recommendations for the user\n",
    "    user_recs = model.recommendForUserSubset(\n",
    "        spark.createDataFrame([(user_id,)], [\"user_id\"]),\n",
    "        num_recommendations\n",
    "    )\n",
    "    \n",
    "    return user_recs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UBCF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding Similar Users: The `find_similar_users` function calculates the similarity between users using a weighted matrix and returns a dataframe of similar users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "import numpy as np\n",
    "\n",
    "def find_similar_users(item_input, n=10, return_dist=False, neg=False):\n",
    "    try:\n",
    "        index = item_input\n",
    "        encoded_index = user_encoder.transform([index])[0]\n",
    "        weights = user_weights\n",
    "        dists = np.dot(weights, weights[encoded_index])\n",
    "        sorted_dists = np.argsort(dists)\n",
    "        n = n + 1\n",
    "        \n",
    "        if neg:\n",
    "            closest = sorted_dists[:n]\n",
    "        else:\n",
    "            closest = sorted_dists[-n:]\n",
    "        \n",
    "        similarity_list = []\n",
    "        for close in closest:\n",
    "            similarity = dists[close]\n",
    "            if isinstance(item_input, int):\n",
    "                decoded_id = user_encoder.inverse_transform([close])[0]\n",
    "                similarity_list.append((decoded_id, similarity))\n",
    "        \n",
    "        # Create a Spark DataFrame\n",
    "        spark = SparkSession.builder.getOrCreate()\n",
    "        schema = [\"similar_users\", \"similarity\"]\n",
    "        df = spark.createDataFrame(similarity_list, schema=schema)\n",
    "        \n",
    "        return df.orderBy(col(\"similarity\"), ascending=False)\n",
    "    except Exception as e:\n",
    "        print(f'\\033[1m{item_input}\\033[0m, Not Found in User list: {e}')\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a random user\n",
    "ratings_per_user = spark_df.groupBy(\"user_id\").count()  # Get number of ratings per user\n",
    "random_user = ratings_per_user.filter(col(\"count\") < 500).orderBy(rand()).select(\"user_id\").limit(1).collect()[0][0]\n",
    "\n",
    "# Find similar users to the random user\n",
    "similar_users_df = find_similar_users(random_user, n=10, neg=False)\n",
    "\n",
    "# Filter users with similarity > 0.4 and exclude the original user\n",
    "similar_users_filtered = similar_users_df.filter((col(\"similarity\") > 0.4) & (col(\"similar_users\") != random_user))\n",
    "\n",
    "# Show results\n",
    "similar_users_filtered.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "User preferences: The `get_user_preferences` function takes a user ID as input and retrieves the anime preferences of that user. It considers the top-rated animes by the user and analyzes the genres they prefer. The function also provides an option to plot a word cloud to visualize the preferred genres. The output is a dataframe containing the anime titles and their associated genres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "# Function to display a word cloud of preferred genres\n",
    "def showWordCloud(all_genres):\n",
    "    genres_cloud = WordCloud(width=700, height=400, background_color='white', colormap='gnuplot').generate_from_frequencies(all_genres)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(genres_cloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "def get_user_preferences(user_id, spark_df, spark_df_anime, plot=False, verbose=0):\n",
    "    # Filter for animes watched by user\n",
    "    animes_watched_by_user = spark_df.filter(F.col('user_id') == user_id)\n",
    "    \n",
    "    # Check if user has watched any animes\n",
    "    if animes_watched_by_user.count() == 0:\n",
    "        print(\"User #{} has not watched any animes.\".format(user_id))\n",
    "        return spark.createDataFrame([], schema=StructType([\n",
    "            StructField(\"Name\", StringType(), True),\n",
    "            StructField(\"Genres\", StringType(), True)\n",
    "        ]))\n",
    "    \n",
    "    # Calculate 75th percentile of user's ratings\n",
    "    user_rating_percentile = animes_watched_by_user.stat.approxQuantile(\"rating\", [0.75], 0.01)[0]\n",
    "    \n",
    "    animes_watched_by_user_top = animes_watched_by_user.filter(F.col(\"rating\") >= user_rating_percentile)\n",
    "    \n",
    "    windowSpec = Window.orderBy(F.desc(\"rating\"))\n",
    "    top_animes_user_df = animes_watched_by_user_top.withColumn(\"rank\", F.row_number().over(windowSpec)) \\\n",
    "                                                 .select(\"anime_id\")\n",
    "    \n",
    "    top_animes_user = [row.anime_id for row in top_animes_user_df.collect()]\n",
    "    \n",
    "    anime_df_rows = spark_df_anime.filter(F.col(\"anime_id\").isin(top_animes_user)) \\\n",
    "                                 .select(\"Name\", \"Genres\")\n",
    "    \n",
    "    if verbose != 0:\n",
    "        avg_rating = animes_watched_by_user.agg(F.mean(\"rating\").alias(\"avg_rating\")).collect()[0][\"avg_rating\"]\n",
    "        \n",
    "        print(\"User \\033[1m{}\\033[0m has watched {} anime(s) with an average rating of {:.1f}/10\\n\".format(\n",
    "            user_id, animes_watched_by_user.count(), avg_rating\n",
    "        ))\n",
    "        print('\\033[1m----- Preferred genres----- \\033[0m\\n')\n",
    "    \n",
    "    if plot:\n",
    "        genres_df = anime_df_rows.select(\"Genres\").collect()\n",
    "        genres_list = []\n",
    "        \n",
    "        for row in genres_df:\n",
    "            genres = row[\"Genres\"]\n",
    "            if genres and isinstance(genres, str):\n",
    "                for genre in genres.split(','):\n",
    "                    genres_list.append(genre.strip())\n",
    "        \n",
    "        showWordCloud(dict(Counter(genres_list)))\n",
    "    \n",
    "    return anime_df_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_user = 12345  # Replace with your actual random_user value\n",
    "user_pref = get_user_preferences(random_user, spark_df, spark_df_anime, plot=True, verbose=1)\n",
    "\n",
    "user_pref.limit(5).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate recommendations for an anime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_anime_recommendations(model, anime_id, num_recommendations=10):\n",
    "    # Get top N users\n",
    "    anime_recs = model.recommendForItemSubset(\n",
    "        spark.createDataFrame([(anime_id,)], [\"anime_id\"]),\n",
    "        num_recommendations\n",
    "    )\n",
    "    \n",
    "    return anime_recs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recommend animes to the selected user based on the preferences of similar users. The `get_recommended_animes` function iterates through the list of similar users, retrieves their preferences, and identifies animes that are not present in the preferences of the selected user. It then generates a list of recommended animes along with their genres and a brief synopsis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recommended_animes(similar_users, user_pref, spark_df, spark_df_anime, n=10):\n",
    "    recommended_animes = []\n",
    "    anime_list = []\n",
    "    \n",
    "    # Extract similar users list (collect to driver)\n",
    "    similar_user_ids = [int(user_id) for user_id in similar_users.select(\"similar_users\").collect()[0][\"similar_users\"]]\n",
    "    \n",
    "    # Get user preferences for each similar user\n",
    "    user_pref_names = [row[\"Name\"] for row in user_pref.select(\"Name\").collect()]\n",
    "    \n",
    "    for user_id in similar_user_ids:\n",
    "        pref_list = get_user_preferences(int(user_id), spark_df, spark_df_anime)\n",
    "        \n",
    "        # Check if user has watched any animes\n",
    "        if pref_list.count() > 0:\n",
    "            # Filter out animes that the target user has already watched\n",
    "            pref_list = pref_list.filter(~F.col(\"Name\").isin(user_pref_names))\n",
    "            \n",
    "            # Collect anime names to driver\n",
    "            anime_names = [row[\"Name\"] for row in pref_list.select(\"Name\").collect()]\n",
    "            if anime_names:\n",
    "                anime_list.append(anime_names)\n",
    "    \n",
    "    if len(anime_list) == 0:\n",
    "        print(\"No anime recommendations available for the given users.\")\n",
    "        return spark.createDataFrame([], schema=StructType([\n",
    "            StructField(\"n\", IntegerType(), True),\n",
    "            StructField(\"anime_name\", StringType(), True),\n",
    "            StructField(\"Genres\", StringType(), True), \n",
    "            StructField(\"Synopsis\", StringType(), True)\n",
    "        ]))\n",
    "    \n",
    "    # This part needs to be done on the driver as it's a flattening operation\n",
    "    # Convert to pandas for the value_counts operation\n",
    "    anime_list_flat = [anime for sublist in anime_list for anime in sublist]\n",
    "    anime_count_series = pd.Series(anime_list_flat).value_counts()\n",
    "    sorted_list = anime_count_series.head(n)\n",
    "    \n",
    "    # Count occurrences of each anime in the entire dataset (using Spark)\n",
    "    anime_count_df = spark_df.groupBy(\"anime_id\").count()\n",
    "    \n",
    "    # Create a broadcast dictionary of anime counts for faster lookups\n",
    "    anime_count_dict = {row[\"anime_id\"]: row[\"count\"] for row in anime_count_df.collect()}\n",
    "    anime_count_broadcast = spark.sparkContext.broadcast(anime_count_dict)\n",
    "    \n",
    "    # Process each recommended anime\n",
    "    for anime_name in sorted_list.index:\n",
    "        if isinstance(anime_name, str):\n",
    "            try:\n",
    "                # Get anime details\n",
    "                anime_row = spark_df_anime.filter(F.col(\"Name\") == anime_name).first()\n",
    "                \n",
    "                if anime_row:\n",
    "                    anime_id = anime_row[\"anime_id\"]\n",
    "                    english_name = anime_row[\"English name\"]\n",
    "                    name = english_name if english_name != \"UNKNOWN\" else anime_name\n",
    "                    genre = anime_row[\"Genres\"]\n",
    "                    synopsis = anime_row[\"Synopsis\"]\n",
    "                    \n",
    "                    # Get count of users who watched this anime\n",
    "                    n_user_pref = anime_count_broadcast.value.get(anime_id, 0)\n",
    "                    \n",
    "                    recommended_animes.append({\n",
    "                        \"n\": n_user_pref,\n",
    "                        \"anime_name\": anime_name,\n",
    "                        \"Genres\": genre,\n",
    "                        \"Synopsis\": synopsis\n",
    "                    })\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing anime {anime_name}: {e}\")\n",
    "                pass\n",
    "    \n",
    "    # Create schema for result DataFrame\n",
    "    schema = StructType([\n",
    "        StructField(\"n\", IntegerType(), True),\n",
    "        StructField(\"anime_name\", StringType(), True),\n",
    "        StructField(\"Genres\", StringType(), True),\n",
    "        StructField(\"Synopsis\", StringType(), True)\n",
    "    ])\n",
    "    \n",
    "    # Create Spark DataFrame from recommended animes\n",
    "    if recommended_animes:\n",
    "        return spark.createDataFrame(recommended_animes, schema)\n",
    "    else:\n",
    "        return spark.createDataFrame([], schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendations = get_recommended_animes(similar_users, user_pref, spark_df, spark_df_anime, n=10)\n",
    "recommendations.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IBCF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StructType, StructField, StringType, ArrayType\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def find_similar_animes_spark(name, spark_df_anime, anime_encoder, anime_weights, n=10, return_dist=False, neg=False):\n",
    "    try:\n",
    "        anime_row = spark_df_anime.filter(F.col(\"Name\") == name).first()\n",
    "        \n",
    "        if not anime_row:\n",
    "            print('{} not found in Anime list'.format(name))\n",
    "            return None\n",
    "            \n",
    "        index = anime_row[\"anime_id\"]\n",
    "        encoded_index = anime_encoder.transform([index])[0]\n",
    "        weights = anime_weights\n",
    "        \n",
    "        dists = np.dot(weights, weights[encoded_index])\n",
    "        sorted_indices = np.argsort(dists)\n",
    "        \n",
    "        n = n + 1\n",
    "        if neg:\n",
    "            closest = sorted_indices[:n]\n",
    "        else:\n",
    "            closest = sorted_indices[-n:]\n",
    "            \n",
    "        print('Animes closest to {}'.format(name))\n",
    "        \n",
    "        if return_dist:\n",
    "            return dists, closest\n",
    "        \n",
    "        similar_animes = []\n",
    "        \n",
    "        dists_broadcast = spark.sparkContext.broadcast(dists)\n",
    "        \n",
    "        decoded_ids = [anime_encoder.inverse_transform([close])[0] for close in closest]\n",
    "        decoded_ids_broadcast = spark.sparkContext.broadcast(decoded_ids)\n",
    "        \n",
    "        schema = StructType([\n",
    "            StructField(\"Name\", StringType(), True),\n",
    "            StructField(\"Similarity\", StringType(), True),\n",
    "            StructField(\"Genres\", StringType(), True),\n",
    "            StructField(\"Synopsis\", StringType(), True)\n",
    "        ])\n",
    "        \n",
    "        filtered_df = spark_df_anime.filter(F.col(\"anime_id\").isin(decoded_ids_broadcast.value))\n",
    "        \n",
    "        similar_animes_rows = filtered_df.collect()\n",
    "        \n",
    "        for row in similar_animes_rows:\n",
    "            anime_id = row[\"anime_id\"]\n",
    "            anime_name = row[\"Name\"]\n",
    "            english_name = row[\"English name\"]\n",
    "            name_to_use = english_name if english_name != \"UNKNOWN\" else anime_name\n",
    "            genre = row[\"Genres\"]\n",
    "            synopsis = row[\"Synopsis\"]\n",
    "            \n",
    "            encoded_id = anime_encoder.transform([anime_id])[0]\n",
    "            similarity = dists_broadcast.value[encoded_id]\n",
    "            similarity_formatted = \"{:.2f}%\".format(similarity * 100)\n",
    "            \n",
    "            similar_animes.append({\n",
    "                \"Name\": name_to_use, \n",
    "                \"Similarity\": similarity_formatted, \n",
    "                \"Genres\": genre, \n",
    "                \"Synopsis\": synopsis\n",
    "            })\n",
    "        \n",
    "        result_df = spark.createDataFrame(similar_animes, schema)\n",
    "        \n",
    "        result_df = result_df.sort(F.col(\"Similarity\").desc()).filter(F.col(\"Name\") != name)\n",
    "        \n",
    "        return result_df\n",
    "    except Exception as e:\n",
    "        print(f'Error: {e}')\n",
    "        print('{} not found in Anime list or other error occurred'.format(name))\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main function to run the recommendation system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function to run the recommendation system\n",
    "def run_anime_recommendation_system(filepath, user_id_to_recommend=None):\n",
    "    print(\"Loading data...\")\n",
    "    full_df, ratings_df = load_anime_data(filepath)\n",
    "    \n",
    "    print(f\"Total ratings: {ratings_df.count()}\")\n",
    "    print(f\"Unique users: {ratings_df.select('user_id').distinct().count()}\")\n",
    "    print(f\"Unique anime: {ratings_df.select('anime_id').distinct().count()}\")\n",
    "    \n",
    "    print(\"Splitting data into training and test sets...\")\n",
    "    train_df, test_df = split_data(ratings_df)\n",
    "    \n",
    "    print(\"Training ALS model...\")\n",
    "    model = train_als_model(train_df)\n",
    "    \n",
    "    print(\"Evaluating model...\")\n",
    "    rmse, predictions = evaluate_model(model, test_df)\n",
    "    print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n",
    "    \n",
    "    if user_id_to_recommend:\n",
    "        print(f\"Generating recommendations for user {user_id_to_recommend}...\")\n",
    "        user_recs = get_user_recommendations(model, user_id_to_recommend)\n",
    "        \n",
    "        user_recs_pd = user_recs.toPandas()\n",
    "        \n",
    "        # Join with anime names\n",
    "        if 'anime_name' in full_df.columns:\n",
    "            anime_names = full_df.select(\"anime_id\", \"anime_name\").distinct()\n",
    "            \n",
    "            # Extract recommendations from the nested structure\n",
    "            from pyspark.sql.functions import explode\n",
    "            user_recs_exploded = user_recs.select(\n",
    "                \"user_id\", \n",
    "                explode(\"recommendations\").alias(\"rec\")\n",
    "            )\n",
    "            user_recs_flattened = user_recs_exploded.select(\n",
    "                \"user_id\", \n",
    "                col(\"rec.anime_id\").alias(\"anime_id\"), \n",
    "                col(\"rec.rating\").alias(\"predicted_rating\")\n",
    "            )\n",
    "            \n",
    "            # Join with anime names\n",
    "            user_recs_with_names = user_recs_flattened.join(\n",
    "                anime_names, \n",
    "                on=\"anime_id\"\n",
    "            ).orderBy(col(\"predicted_rating\").desc())\n",
    "            \n",
    "            print(\"Top recommended anime:\")\n",
    "            user_recs_with_names.show(10, truncate=False)\n",
    "    \n",
    "    return model, full_df, ratings_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example use case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Placeholder for path\n",
    "    filepath = \"anime_ratings_2023.csv\"\n",
    "    \n",
    "    model, full_df, ratings_df = run_anime_recommendation_system(filepath, user_id_to_recommend=123)\n",
    "    \n",
    "    # Optional: Save the model\n",
    "    model.save(\"anime_als_model\")\n",
    "    \n",
    "    spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Completed tasks:\n",
    "- Setting up the recommendation system models\n",
    "- Read datasets and train/test split \n",
    "- Explore the reason to use the ALS model \n",
    "- Integrated the processed datasets (anime, user-rating) to the model \n",
    "- Setting up UBCF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remaining tasks:\n",
    "- Integrating Spark with Redis database\n",
    "- Solve the problem with real-time data on Redis while using ALS with micro-batch processing\n",
    "- Training the model, evaluation of the model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
