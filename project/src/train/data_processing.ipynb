{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/05 20:59:24 WARN Utils: Your hostname, p4stwi2x resolves to a loopback address: 127.0.1.1; using 192.168.100.188 instead (on interface wlp2s0)\n",
      "25/04/05 20:59:24 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/04/05 20:59:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "dataset_input_dir = \"dataset/\"\n",
    "dataset_output_dir = \"\"\n",
    "show_parts = False\n",
    "\n",
    "user_dataset_input = dataset_input_dir + \"users-details-2023.csv\"                                                                                                                                                                                                   #p@stwi2x\n",
    "anime_dataset_input = dataset_input_dir + \"anime-dataset-2023.csv\"\n",
    "unified_dataset_input = dataset_input_dir + \"users-score-2023_short.csv\"\n",
    "\n",
    "anime_dataset_output = dataset_output_dir + \"anime_embeddings.csv\"\n",
    "user_dataset_output = dataset_output_dir + \"user_embeddings.csv\"\n",
    "# unified_dataset_output = dataset_output_dir + \"unified_dataset_prep.csv\"\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession\\\n",
    "    .builder \\\n",
    "    .appName(\"Spark SQL app\")\\\n",
    "    .config('spark.eventLog.gcMetrics.youngGenerationGarbageCollectors', 'true')\\\n",
    "    .config('spark.driver.memory', '16g')\\ \n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ALWAYS run Related functions before proceeding.\n",
    "\n",
    "Choose 1 out of the 2 options:\n",
    "- Hit \"Run All\" and start from the original datasets (check configs).\n",
    "- Scroll down to Main Dataset section, there is option to load pre-prepped datasets, run the codes starting from there after configurations.\n",
    "\n",
    "For easier readability: search \"VS code jupyter notebook collapse all\".\n",
    "\n",
    "In order to shorten time, use flag show_parts = True.\n",
    "\n",
    "NOTE: In my results, the unified dataset contains 23419469 rows x 155 columns, assuming MAX 4 bytes (float) per data, is around 14.5 GB MAX. So make sure you have at least 15 GB if you intend to output the unified dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Related functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Simple functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_shape(dataFrame):\n",
    "    print(dataFrame.count(), len(dataFrame.columns))\n",
    "def unique_categories(dataFrame, colName, show: bool = True):\n",
    "    L = dataFrame.select(colName).rdd.flatMap(lambda x: x).collect()\n",
    "    valueSet = {}\n",
    "    for i in L:\n",
    "        if i is not None:\n",
    "            temp = i.split(\", \")\n",
    "            for j in temp:\n",
    "                if colName + \"_\" + j not in valueSet.keys():\n",
    "                    valueSet.update({colName + \"_\" + j: 1})\n",
    "                else:\n",
    "                    valueSet[colName + \"_\" + j] += 1\n",
    "    if show:\n",
    "        print(valueSet)\n",
    "    return valueSet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dataset column syntax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_syntax(dataFrame):\n",
    "    # replace ' ' with '_'\n",
    "    to_change = {}\n",
    "    column_order = list(dataFrame.columns)\n",
    "    for i in range(len(column_order)):\n",
    "        if ' ' in column_order[i]:\n",
    "            new_name = column_order[i].replace(' ', '_')\n",
    "\n",
    "            to_change[column_order[i]] = new_name\n",
    "            column_order[i] = new_name\n",
    "\n",
    "    dataFrame = dataFrame.withColumnsRenamed(to_change)\n",
    "\n",
    "    # replace other symbols\n",
    "    import re\n",
    "    to_change = {}\n",
    "    for i in range(len(column_order)):\n",
    "        new_name = re.sub(\"[!@#$%^&*().,']\",\"\", column_order[i]) \n",
    "\n",
    "        if new_name != column_order[i]:\n",
    "            to_change[column_order[i]] = new_name\n",
    "            column_order[i] = new_name\n",
    "\n",
    "    dataFrame = dataFrame.withColumnsRenamed(to_change)\n",
    "\n",
    "    return dataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### One-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cre: https://stackoverflow.com/questions/72600575/one-hot-encoding-to-a-list-feature-pyspark\n",
    "import gc\n",
    "from pyspark.sql.functions import array_contains, split, col, explode\n",
    "def oneHotEncoding(dataFrame, colName: str, show: bool = True, support: int = 0):\n",
    "    size_before = dataFrame.count()\n",
    "\n",
    "    test_df = dataFrame.select(colName)\n",
    "    if show:\n",
    "        print(f\"Fill rate: {test_df.filter(col(colName).isNotNull()).count()}/{test_df.count()}\")\n",
    "\n",
    "    # collect list of unique genres\n",
    "    test_df = test_df.select('*', split(test_df[colName], ',').alias('tags'))\n",
    "    \n",
    "    tags = [x[0] \n",
    "        for x in \n",
    "        test_df.select(explode('tags')).collect()\n",
    "    ]\n",
    "\n",
    "    # support check\n",
    "    tags_count = {}\n",
    "    for tag in tags:\n",
    "        if tag not in tags_count.keys():\n",
    "            tags_count[tag] = 0\n",
    "        tags_count[tag] += 1\n",
    "\n",
    "    tag_list = list(tags_count.keys())\n",
    "    for tag in tag_list:\n",
    "        if tags_count[tag] < support:\n",
    "            tags_count.pop(tag, None)\n",
    "\n",
    "    tags = tags_count.keys() \n",
    "\n",
    "    # transfer unique values to panda DF to map the entries\n",
    "    test_df = test_df.select(colName,\n",
    "        *[\n",
    "            array_contains('tags', tag).alias((colName + \"_{}\").format(tag)).cast(\"integer\")\\\n",
    "            for tag in tags\n",
    "        ]\n",
    "    ).dropDuplicates()\n",
    "\n",
    "    result_df = dataFrame.join(test_df, dataFrame[colName] == test_df[colName], 'left')\\\n",
    "        .drop(colName)\n",
    "\n",
    "    # store the embedding column list\n",
    "\n",
    "    return result_df, tags\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Min max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import min, max, col\n",
    "def min_max(dataFrame, colName_in: str, colName_out: str):\n",
    "    temp_df = dataFrame.select(min(colName_in), max(colName_in)).collect()\n",
    "    min_val, max_val = float(temp_df[0][0]), float(temp_df[0][1])\n",
    "\n",
    "    dataFrame = dataFrame.withColumn(colName_out, (col(colName_in)-min_val)/(max_val-min_val))                                                                                                                                                                                         #thien7170\n",
    "\n",
    "    return dataFrame, list([min_val, max_val])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import min, max\n",
    "import pandas as pd\n",
    "\n",
    "def plot(dataFrame, colName: str):\n",
    "    gre_histogram = dataFrame.select(colName).filter(col(colName).cast(\"float\").isNotNull())\\\n",
    "        .rdd.flatMap(lambda x: x)\\\n",
    "        .map(float)\\\n",
    "        .histogram(20)\n",
    "\n",
    "    pd.DataFrame(\n",
    "        list(zip(*gre_histogram)), \n",
    "        columns=['bin', 'frequency']\n",
    "    ).set_index('bin').plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "user_df = spark.read\\\n",
    "    .option(\"header\", True)\\\n",
    "    .option(\"escape\", '\"')\\\n",
    "    .option(\"multiline\", True)\\\n",
    "    .option(\"sep\", \",\")\\\n",
    "    .csv(user_dataset_input)\n",
    "\n",
    "user_embed_columns = {}\n",
    "\n",
    "# element: UNKNOWN\n",
    "user_df = user_df.replace({'UNKNOWN': None, 'Unknown': None}).dropDuplicates()\n",
    "\n",
    "if show_parts:\n",
    "    start_X, start_Y = user_df.count(), len(user_df.columns)\n",
    "    print_shape(user_df)\n",
    "    user_df.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check sparsity for each column\n",
    "from pyspark.sql.functions import col\n",
    "from collections import OrderedDict\n",
    "\n",
    "if show_parts:\n",
    "    rows = user_df.count()\n",
    "    sparsity_score = [user_df.filter(col(Col).isNotNull()).count() / rows for Col in user_df.columns]\n",
    "\n",
    "    sparsity_list = OrderedDict()\n",
    "    for i in range(len(user_df.columns)):\n",
    "        sparsity_list[user_df.columns[i]] = sparsity_score[i]\n",
    "    #spark.createDataFrame([d_list]).show()\n",
    "    print(sparsity_list.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Username"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This column is just user's username as strings, should be dropped to make the dataset lighter and we already have Mal ID as user index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_df = user_df.drop('Username')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gender"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As this categorical column has 2 values at most, we perform one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/05 20:38:10 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Young Generation), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n",
      "25/04/05 20:38:10 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC, G1 Young Generation), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "user_df, col_list = oneHotEncoding(user_df, 'Gender', show=show_parts)\n",
    "user_embed_columns['Gender'] = col_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Birthday"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shall convert the non-null values to unix timestamps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split, unix_timestamp\n",
    "split_col = split(user_df['Birthday'], 'T', 2)\n",
    "user_df = user_df.withColumn('Birthday_d', split_col.getItem(0))\\\n",
    "    .withColumn('Birthday_unix', unix_timestamp('Birthday_d', format='yyyy-mm-dd'))\\\n",
    "    .drop('Birthday', 'Birthday_d')\\\n",
    "    .withColumnRenamed('Birthday_unix', 'Birthday')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if show_parts:\n",
    "    user_df.select('Birthday').where('Birthday is not NULL').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is worth noting that anime titles are vastly different for users of different ages, that is why we shall perform user age when giving rating.\n",
    "\n",
    "However, for the time being, we shall perform a simple plotting and eliminating outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "user_df, _ = min_max(user_df, 'Birthday', 'Birthday_m')\n",
    "user_df = user_df.drop('Birthday')\\\n",
    "    .withColumnRenamed('Birthday_m', 'Birthday')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if show_parts:\n",
    "    print_shape(user_df)\n",
    "    plot(user_df, 'Birthday')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "user_df = user_df.filter('Birthday is NULL OR Birthday >= 0.9')\n",
    "user_df, _ = min_max(user_df, 'Birthday', 'Birthday_m')\n",
    "user_df = user_df.drop('Birthday')\\\n",
    "    .withColumnRenamed('Birthday_m', 'Birthday')\n",
    "if show_parts:\n",
    "    print_shape(user_df)\n",
    "    plot(user_df, 'Birthday')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "user_df = user_df.filter('Birthday is NULL OR (Birthday >= 0.4 AND Birthday < 0.5)')\n",
    "user_df, user_embed_columns['Birthday'] = min_max(user_df, 'Birthday', 'Birthday_m')\n",
    "user_df = user_df.drop('Birthday')\\\n",
    "    .withColumnRenamed('Birthday_m', 'Birthday')\n",
    "if show_parts:\n",
    "    print_shape(user_df)\n",
    "    plot(user_df, 'Birthday')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examining the skewness of the observed data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import mean, median\n",
    "if show_parts:\n",
    "    user_df.select(mean('Birthday'), median('Birthday')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the metrics are very close to each other, the skewness of the observed data should not be a concern for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if show_parts:\n",
    "    print_shape(user_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Location (finished)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "if show_parts:\n",
    "    user_df.select('Location').where('Location is not NULL').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the location format is very inconsistent, going back and forth between cities and/or countries and imcomprehensible values. \n",
    "\n",
    "It is better for us to drop this column for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_df = user_df.drop('Location')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Joined (finished)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the date when the user joins the MyAnimeList platform. If we intend to use this data to check user's anime-watching duration to give more weights to those who watch more and are able to rate titles more objectively, it shall be better to use the column \"Days Watched\" instead.\n",
    "\n",
    "Hence, this column is dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_df = user_df.drop('Joined')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Days Watched (finished)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This column shows the number of days user has spent watching anime. The more experience one has with watching, one could judge and rate anime titles more accurately.\n",
    "\n",
    "Hence, we use this column, with the name changed for easier SQL queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_df = user_df.withColumnRenamed('Days Watched', 'Days_Watched')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The more time one spends watching anime, the more they can observe and rate anime titles more correctly. And the difference between experienced watchers (such as 1000 and 2000 hours) is not as much as between inexperienced (10 hours) and experienced (500 hours).\n",
    "\n",
    "We shall perform log transformation then standardization and do not perform any outlier elimination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import log10, col\n",
    "user_df = user_df.withColumn('Days_Watched_log', log10(col('Days_Watched')+1.0))\n",
    "user_df = user_df.drop('Days_Watched')\\\n",
    ".withColumnRenamed('Days_Watched_log', 'Days_Watched')\n",
    "\n",
    "user_df, user_embed_columns['Days_Watched'] = min_max(user_df, 'Days_Watched', 'A1')\n",
    "user_df = user_df.drop('Days_Watched').withColumnRenamed('A1', 'Days_Watched')\n",
    "\n",
    "if show_parts:\n",
    "    user_df.select('Days_Watched').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "if show_parts:\n",
    "    print_shape(user_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mean Score (finished)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~This column is mean to see how user rates compared to how they usually rate, so it shall be left untouched until joining the datasets into one unified file.~~\n",
    "\n",
    "We perform min-max on this, as CF NN model does not require us to handle the average problem, yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "user_df, _ = min_max(user_df, 'Mean Score', 'Mean Score_k')\n",
    "\n",
    "user_df = user_df.drop('Mean Score').withColumnRenamed('Mean Score_k', 'Mean Score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Watching, On Hold, Dropped, Plan to Watch, Total Entries (finished)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These columns are to see how the user interacts with anime titles.\n",
    "\n",
    "However, to us, these columns do not mean a lot about user's rating sincerity as we are recommending anime based on tastes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_df = user_df.drop('Watching', 'On Hold', 'Dropped', 'Plan to Watch', 'Total Entries')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Completed, Rewatched, Episodes Watched (finished)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These columns are important in determining user's experience in watching anime, similar to Days Watched. However, since Rewatched only counts number of titles a user has rewatched, without pointing out which titles exactly, and furthermore is similar to, and counted within, the Episodes Watched column.\n",
    "\n",
    "Our decision is to drop Rewatched, and keep the other two, like Days Watched: log transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "user_df = user_df.drop('Rewatched')\\\n",
    "    .withColumnRenamed('Episodes Watched', 'Episodes_Watched')\n",
    "\n",
    "a, b = 'Episodes_Watched', 'Completed'\n",
    "\n",
    "user_df = user_df.withColumn(a + '_l', log10(col(a)+1.0))\\\n",
    "    .withColumn(b+ '_l', log10(col(b)+1.0))\\\n",
    "    .drop(a, b)\\\n",
    "    .withColumnRenamed(a+'_l', a)\\\n",
    "    .withColumnRenamed(b+'_l', b)\n",
    "\n",
    "user_df, user_embed_columns[b] = min_max(user_df, b, b+'_m')\n",
    "user_df, user_embed_columns[a] = min_max(user_df, a, a+'_m')\n",
    "user_df = user_df.drop(a,b)\\\n",
    "    .withColumnRenamed(a+'_m', a)\\\n",
    "    .withColumnRenamed(b+'_m', b)\n",
    "\n",
    "if show_parts:\n",
    "    user_df.select(a,b).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "if show_parts:\n",
    "    print_shape(user_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "User embeddings dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/05 20:40:34 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "user_df.coalesce(1).write\\\n",
    "        .option(\"header\", \"true\")\\\n",
    "        .option(\"sep\", \",\")\\\n",
    "        .mode(\"overwrite\")\\\n",
    "        .csv(user_dataset_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedded columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "\n",
    "# with open('user_embed_col.json', 'w') as fp:\n",
    "#     json.dump(user_embed_columns, fp, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anime dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "anime_df = spark.read\\\n",
    "    .option(\"header\", True)\\\n",
    "    .option(\"escape\", '\"')\\\n",
    "    .option(\"multiline\", True)\\\n",
    "    .option(\"sep\", \",\")\\\n",
    "    .csv(anime_dataset_input)\n",
    "\n",
    "anime_embed_columns = {}\n",
    "\n",
    "# element: UNKNOWN\n",
    "anime_df = anime_df.replace({'UNKNOWN': None, 'Unknown': None})\n",
    "anime_df.dropDuplicates()\n",
    "\n",
    "# change column names for better SQL\n",
    "anime_df = anime_df.withColumnRenamed('Scored By', 'Scored_By')\n",
    "\n",
    "if show_parts:\n",
    "    start_X, start_Y = anime_df.count(), len(anime_df.columns)\n",
    "    print_shape(anime_df)\n",
    "    anime_df.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check sparsity for each column\n",
    "from pyspark.sql.functions import col\n",
    "from collections import OrderedDict\n",
    "\n",
    "if show_parts:\n",
    "    rows = anime_df.count()\n",
    "    sparsity_score = [anime_df.filter(col(Col).isNotNull()).count() / rows for Col in anime_df.columns]\n",
    "\n",
    "    sparsity_list = OrderedDict()\n",
    "    for i in range(len(anime_df.columns)):\n",
    "        sparsity_list[anime_df.columns[i]] = sparsity_score[i]\n",
    "    #spark.createDataFrame([d_list]).show()\n",
    "    sparsity_list.items()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Name, English name, Other name (finished)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the problem is about recommendation based on user's taste, it is expected that these features have nothing to do with our recommendation system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "anime_df = anime_df.drop(\"Name\", \"English name\", \"Other name\")\n",
    "if show_parts:\n",
    "    anime_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24905 21\n"
     ]
    }
   ],
   "source": [
    "print_shape(anime_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Score (finished)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As this is a numerical column, we perform data normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "anime_df, anime_embed_columns['Score'] = min_max(anime_df, \"Score\", \"Score_s\")\n",
    "\n",
    "anime_df = anime_df.drop('Score').withColumnRenamed(existing=\"Score_s\",new=\"Score\")\n",
    "\n",
    "if show_parts:\n",
    "    plot(anime_df, \"Score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "if show_parts:\n",
    "    anime_df.select(\"Score\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import median, mode\n",
    "if show_parts:\n",
    "    anime_df.select(median(\"Score\"), mode(\"Score\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though the data is negatively skewed (mean = 0, median = 0.009, mode = 0.171), the skewed-ness is insignificant and can be allowed for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "if show_parts:\n",
    "    print_shape(anime_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Genres (finished)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the column is categorical and the label order in Genres is not important, we use one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "anime_df, col_list = oneHotEncoding(anime_df, \"Genres\", show=show_parts)\n",
    "anime_embed_columns['Genres'] = col_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "if show_parts:\n",
    "    print_shape(anime_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Synopsis (finished)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Synopsis is description of an anime title and has quite some impact to user's taste. However we temporarily leave this out for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "anime_df = anime_df.drop(\"Synopsis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Type (finished)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to Genres, we use one-hot encoding here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "if show_parts:\n",
    "    anime_df.select(\"Type\").groupBy(\"Type\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since only 0.3% of samples have empty values, so we should have no problem dropping these rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/05 20:40:56 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|   Type|count|\n",
      "+-------+-----+\n",
      "|     TV| 7597|\n",
      "|Special| 2558|\n",
      "|    OVA| 4076|\n",
      "|  Music| 2686|\n",
      "|  Movie| 4381|\n",
      "|    ONA| 3533|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "anime_df = anime_df.filter(\"Type is not NULL\")\n",
    "anime_df.select(\"Type\").groupBy(\"Type\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we perform one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fill rate: 24831/24831\n"
     ]
    }
   ],
   "source": [
    "anime_df, col_list = oneHotEncoding(anime_df, \"Type\")\n",
    "user_embed_columns['Type'] = col_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24831 65\n"
     ]
    }
   ],
   "source": [
    "print_shape(anime_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Episodes (finished)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anime length (no. of episodes and duration) might have to do with watching trends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "anime_df, anime_embed_columns['Episodes'] = min_max(anime_df, \"Episodes\", \"A1\")\n",
    "anime_df = anime_df.drop('Episodes').withColumnRenamed(\"A1\", \"Episodes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aired (finished)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The format is in the form of \"(start date) to (end date)\", so we perform splitting into two columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# column: Aired \n",
    "from pyspark.sql.functions import split, to_date\n",
    "\n",
    "split_col = split(anime_df[\"Aired\"], \" to \", 2)\n",
    "anime_df = anime_df.withColumn(\"F1\", split_col.getItem(0)).withColumn(\"F2\", split_col.getItem(1))\n",
    "anime_df = anime_df.withColumn(\"air_start\", to_date(col(\"F1\"), \"MMM d, yyyy\"))\\\n",
    "    .withColumn(\"air_end\", to_date(col(\"F2\"), \"MMM d, yyyy\"))\\\n",
    "    .drop(\"F1\", \"F2\", \"Aired\")\n",
    "if show_parts:\n",
    "    anime_df.select(\"air_start\", \"air_end\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some cases, some titles do not have both timestamps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "if show_parts:\n",
    "    print(anime_df.select(\"air_start\", \"air_end\").where(\"air_start is not NULL\").count())\n",
    "    print(anime_df.select(\"air_start\", \"air_end\").where(\"air_end is not NULL\").count())\n",
    "    print(anime_df.select(\"air_start\", \"air_end\").where(\"air_start is not NULL AND air_end is not NULL\").count())                                                               #p4stwi2x\n",
    "    print(anime_df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the number of missing values is huge for both columns and users can manually filter the recommendation rankings, our choice is to drop the columns in processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "anime_df = anime_df.drop(\"air_start\", \"air_end\")\n",
    "if show_parts:\n",
    "    anime_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Premiered (finished)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since some users might have the tendency to watch the latest titles, and so as some for the older titles for nostalgia, trendy eras, etc., we shall use this feature. The feature is in the form of season and year, which we shall transform into real numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split, col\n",
    "\n",
    "if show_parts:\n",
    "    print(f\"Column sparsity: {sparsity_list[\"Premiered\"]}\")\n",
    "\n",
    "split_col = split(anime_df[\"Premiered\"], \" \", 2)\n",
    "anime_df = anime_df.withColumn(\"F1\", split_col.getItem(0)).withColumn(\"F2\", split_col.getItem(1))\n",
    "anime_df = anime_df.na.replace(['spring', 'summer', 'fall', 'winter'], ['0', '0.25', '0.5', '0.75'], 'F1')\\\n",
    "    .withColumn(\"Premiered_f\", col('F1')+col('F2')).drop(\"Premiered\", \"F1\", \"F2\")\n",
    "\n",
    "if show_parts:\n",
    "    plot(anime_df, \"Premiered_f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import mean, median, mode\n",
    "anime_df, anime_embed_columns['Premiered'] = min_max(anime_df, \"Premiered_f\", \"Premiered_fo\")\n",
    "\n",
    "if show_parts:\n",
    "    anime_df.select(median(\"Premiered_fo\"), mode(\"Premiered_fo\")).show()\n",
    "\n",
    "anime_df = anime_df.drop(\"Premiered\", \"Premiered_f\").withColumnRenamed(existing=\"Premiered_fo\", new=\"Premiered\")\n",
    "if show_parts:\n",
    "    plot(anime_df, \"Premiered\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Status (finished)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List of unique values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Status_Finished Airing': 24063,\n",
       " 'Status_Currently Airing': 347,\n",
       " 'Status_Not yet aired': 421}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_categories(anime_df, \"Status\", show=show_parts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the list is short, we shall perform one-hot encoding with no support without any issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "anime_df, col_list = oneHotEncoding(anime_df, \"Status\", show=show_parts)\n",
    "anime_embed_columns['Status'] = col_list\n",
    "if show_parts:\n",
    "    anime_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Producers (finished)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "uVal = unique_categories(anime_df, \"Producers\", show=show_parts)\n",
    "if show_parts:\n",
    "    print(uVal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Too many columns so we have to add support. Firstly we have to plot the frequencies of the categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "if show_parts:\n",
    "    uVal_df = spark.createDataFrame(pd.DataFrame(list(uVal.values()), columns=['col1']))\n",
    "    plot(uVal_df, \"col1\")\n",
    "    plot(uVal_df.where(\"col1 > 60\"), \"col1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "if show_parts:\n",
    "    print(uVal_df.count())\n",
    "    print(uVal_df.where(\"col1 >= 100\").count())\n",
    "    print(uVal_df.where(\"col1 >= 150\").count())\n",
    "    print(uVal_df.where(\"col1 >= 200\").count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we shall get support = 100 for 56 entries, in order to avoid having too much features causing overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "anime_df, col_list = oneHotEncoding(anime_df, \"Producers\", show=show_parts, support=100)\n",
    "anime_embed_columns['Producers'] = col_list\n",
    "if show_parts:\n",
    "    anime_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Licensors (finished)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "if show_parts:\n",
    "    uVal = unique_categories(anime_df, \"Licensors\", show=show_parts)\n",
    "    print(len(uVal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "if show_parts:\n",
    "    uVal_df = spark.createDataFrame(pd.DataFrame(list(uVal.values()), columns=['col1']))\n",
    "    plot(uVal_df, \"col1\")\n",
    "    plot(uVal_df.where(\"col1 < 65\"), \"col1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "if show_parts:\n",
    "    print(uVal_df.count())\n",
    "    print(uVal_df.where(\"col1 >= 30\").count())\n",
    "    print(uVal_df.where(\"col1 >= 50\").count())\n",
    "    print(uVal_df.where(\"col1 >= 100\").count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "anime_df, col_list = oneHotEncoding(anime_df, \"Licensors\", show=show_parts, support=100)\n",
    "anime_embed_columns['Licensors'] = col_list\n",
    "if show_parts:\n",
    "    anime_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Studios (finished)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "if show_parts:\n",
    "    uVal = unique_categories(anime_df, \"Studios\", False)\n",
    "    print(len(uVal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "if show_parts:\n",
    "    uVal_df = spark.createDataFrame(pd.DataFrame(list(uVal.values()), columns=['col1']))\n",
    "    plot(uVal_df, \"col1\")\n",
    "    plot(uVal_df.where(\"col1 > 100\"), \"col1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "if show_parts:\n",
    "    print(uVal_df.count())\n",
    "    print(uVal_df.where(\"col1 >= 100\").count())\n",
    "    print(uVal_df.where(\"col1 >= 150\").count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "anime_df, anime_embed_columns['Studios'] = oneHotEncoding(anime_df, \"Studios\", show=show_parts, support=100)\n",
    "if show_parts:\n",
    "    anime_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Source (finished)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "if show_parts:\n",
    "    uVal = unique_categories(anime_df, \"Source\", False)\n",
    "    print(len(uVal))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of categories is small, so we do not reduce."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "anime_df, col_list = oneHotEncoding(anime_df, \"Source\", show=show_parts, support=100)\n",
    "anime_embed_columns['Source'] = col_list\n",
    "if show_parts:\n",
    "    anime_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Duration (finished)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The format is \"? hr ? min (per ep)\", we shall convert to a single number representing the duration for each episode in minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "temp_df = anime_df.select(\"Duration\")\n",
    "#temp_df.show()\n",
    "\n",
    "data = temp_df.toPandas()[\"Duration\"].tolist()\n",
    "new_d = []\n",
    "for i in range(len(data)):\n",
    "    if data[i] is not None:\n",
    "        S = data[i].split(\" per ep\")[0]\n",
    "        if \"min\" not in S:\n",
    "            if \"hr\" in S:\n",
    "                S += \" 0 min\"\n",
    "            else:\n",
    "                S = \"0 min \" + S\n",
    "        if \"hr\" not in S:\n",
    "            S = \"0 hr \" + S\n",
    "        if \"sec\" not in S:\n",
    "            S += \" 0 sec\"\n",
    "        \n",
    "        h, m, s = S.split(\" hr \")[0], S.split(\" hr \")[1].split(\" min \")[0], S.split(\" hr \")[1].split(\" min \")[1].split(\" sec\")[0]\n",
    "        new_d.append(int(h)*3600+int(m)*60+int(s))\n",
    "    else:\n",
    "        new_d.append(None)\n",
    "if show_parts:\n",
    "    print(len(new_d))\n",
    "data = spark.createDataFrame(pd.DataFrame({\n",
    "    'Duration': data,\n",
    "    'Duration_f': new_d\n",
    "})).dropDuplicates()\n",
    "\n",
    "anime_df = anime_df.join(data, 'Duration', 'left').drop(\"Duration\").withColumnRenamed(existing=\"Duration_f\", new=\"Duration\").dropDuplicates()                   #p4stwi2x\n",
    "if show_parts:\n",
    "    anime_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "if show_parts:\n",
    "    anime_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rating (finished)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This feature shall be used to filter the recommendation list by user's age and/or filter options, hence it is not to be in the processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "anime_df = anime_df.drop(\"Rating\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rank (finished)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the author: \"The rank of the anime based on popularity or other criteria\". Since the metric is not clear and we are here to predict the ranking by ourselves, so it is better to have this feature removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "anime_df = anime_df.drop(\"Rank\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Popularity (finished)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the author: \"The popularity rank of the anime\". It is indeed that the more popular titles are likely to be watched by users, so they should appear higher on our predicted ranking list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "if show_parts:\n",
    "    plot(anime_df, \"Popularity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, it is indeed that this column is just the ranking for the titles, which do not have much value in determining the actual differences between the popularity of two different anime titles. So it is better for us to drop this and come up with a different way to get the popularity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "anime_df = anime_df.drop(\"Popularity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Favorites, Scored By, Members (finished)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we mentioned with the Popularity column, we shall find ways to represent an anime title's popularity. That is why we keep these 3 features, using log normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in ['Favorites', 'Scored_By', 'Members']:\n",
    "    anime_df = anime_df.withColumn(i+'_l', log10(col(i)+1.0))\\\n",
    "        .drop(i).withColumnRenamed(existing=i+'_l', new=i)             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This feature suggests the number of times an anime title is scored. As explained from the Popularity feature, this shall determine how popular a title is to be better on our ranking list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Image URL (finished)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature has nothing to do with data processing, so the column is expected to be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "anime_df = anime_df.drop(\"Image URL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "anime_df.coalesce(1).write\\\n",
    "        .option(\"header\", \"true\")\\\n",
    "        .option(\"sep\", \",\")\\\n",
    "        .mode(\"overwrite\")\\\n",
    "        .csv(anime_dataset_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedded columns list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "\n",
    "# with open('anime_embed_col.json', 'w') as fp:\n",
    "#     json.dump(anime_embed_columns, fp, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optional: Read pre-prepped side datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: \n",
    "- Ctrl+A to choose all code lines.\n",
    "- Ctrl+K then Ctrl+C to comment all lines selected.\n",
    "- Ctrl+K then Ctrl+U to uncomment all lines selected.\n",
    "- Change the csv paths ONLY."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "anime_df = spark.read\\\n",
    "    .option(\"header\", True)\\\n",
    "    .option(\"escape\", '\"')\\\n",
    "    .option(\"multiline\", True)\\\n",
    "    .option(\"sep\", \",\")\\\n",
    "    .csv(\"anime_embeddings.csv\")\n",
    "user_df = spark.read\\\n",
    "    .option(\"header\", True)\\\n",
    "    .option(\"escape\", '\"')\\\n",
    "    .option(\"multiline\", True)\\\n",
    "    .option(\"sep\", \",\")\\\n",
    "    .csv(\"user_embeddings.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorization of user and anime embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import array, col\n",
    "user_embeddings = user_df.select('Mal ID', array([col(column) for column in user_df.columns if column != 'Mal ID']).alias('user_encoded'))\n",
    "user_embeddings_size = len(user_df.columns) - 1\n",
    "\n",
    "anime_embeddings = anime_df.select('anime_id', array([col(column) for column in anime_df.columns if column != 'anime_id'])\\\n",
    "    .alias('anime_encoded'))\n",
    "anime_embeddings_size = len(anime_df.columns) - 1\n",
    "if show_parts:\n",
    "    user_embeddings.show(5,False)\n",
    "    anime_embeddings.show(5,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read main dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_df = spark.read\\\n",
    "    .option(\"header\", True)\\\n",
    "    .option(\"escape\", '\"')\\\n",
    "    .option(\"multiline\", True)\\\n",
    "    .option(\"sep\", \",\")\\\n",
    "    .csv(unified_dataset_input)\n",
    "\n",
    "rating_df.dropDuplicates()\n",
    "\n",
    "if show_parts:\n",
    "    start_X, start_Y = rating_df.count(), len(rating_df.columns)\n",
    "    print_shape(rating_df)\n",
    "    #main_df.show(20) #This takes a while so by default is commented out.\n",
    "\n",
    "num_users = rating_df.select('user_id').distinct().count()\n",
    "num_animes = rating_df.select('anime_id').distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset limitation for testing purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rating_df.limit(10000).coalesce(1).write\\\n",
    "#         .option(\"header\", \"true\")\\\n",
    "#         .option(\"sep\", \",\")\\\n",
    "#         .mode(\"overwrite\")\\\n",
    "#         .csv(unified_dataset_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from collections import OrderedDict\n",
    "\n",
    "if show_parts:\n",
    "    rows = rating_df.count()\n",
    "    sparsity_score = [rating_df.filter(col(Col).isNotNull()).count() / rows for Col in rating_df.columns]\n",
    "\n",
    "    sparsity_list = OrderedDict()\n",
    "    for i in range(len(rating_df.columns)):\n",
    "        sparsity_list[rating_df.columns[i]] = sparsity_score[i]\n",
    "    print(sparsity_list.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Columns: Username, Anime Title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we already have the two ID columns, we shall remove these string columns to make the dataset far lighter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_df = rating_df.drop('Username', 'Anime Title')\n",
    "\n",
    "if show_parts:\n",
    "    print_shape(rating_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Datasets: Join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We perform joining the embedding datasets into one unified dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df = rating_df\\\n",
    "    .join(user_embeddings, rating_df['user_id'] == user_embeddings['Mal ID'], 'left')\\\n",
    "    .join(anime_embeddings, rating_df['anime_id'] == anime_embeddings['anime_id'], 'left')\n",
    "\n",
    "main_df = main_df.drop(user_embeddings['Mal ID'], anime_embeddings['anime_id'])\n",
    "if show_parts:\n",
    "    print_shape(main_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving columns (used for future embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "\n",
    "# with open('unified_embed_col.json', 'w') as fp:\n",
    "#     json.dump(dict({'unified_embed': list(main_df.columns)}), fp, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving short version of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: \n",
    "- Ctrl+A to choose all code lines.\n",
    "- Ctrl+K then Ctrl+C to comment all lines selected.\n",
    "- Ctrl+K then Ctrl+U to uncomment all lines selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp_df = main_df.limit(10000)\n",
    "# temp_df.coalesce(1).write\\\n",
    "#         .option(\"header\", \"true\")\\\n",
    "#         .option(\"sep\", \",\")\\\n",
    "#         .mode(\"overwrite\")\\\n",
    "#         .csv(unified_dataset_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving full data (very, very long > 8 mins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: \n",
    "- Ctrl+A to choose all code lines.\n",
    "- Ctrl+K then Ctrl+C to comment all lines selected.\n",
    "- Ctrl+K then Ctrl+U to uncomment all lines selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main_df.coalesce(1).write\\\n",
    "#         .option(\"header\", \"true\")\\\n",
    "#         .option(\"sep\", \",\")\\\n",
    "#         .mode(\"overwrite\")\\\n",
    "#         .csv(unified_dataset_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if show_parts:\n",
    "    print_shape(main_df)\n",
    "    main_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "main_df = main_df.limit(1000)\n",
    "num_users = main_df.select(\"user_id\").distinct().count()\n",
    "num_animes = main_df.select('anime_id').distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset split train + test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples in the training set: 814\n",
      "Number of samples in the test set: 186\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import rand\n",
    "def randomize_and_split(df, test_size=0.2, seed=100):\n",
    "\n",
    "    df_shuffled = df.orderBy(rand(seed=seed)).cache()\n",
    "\n",
    "    train_df, test_df = df_shuffled.randomSplit([1 - test_size, test_size], seed=seed)\n",
    "    print(\"Number of samples in the training set:\", train_df.count())\n",
    "    print(\"Number of samples in the test set:\", test_df.count())\n",
    "    return train_df, test_df\n",
    "\n",
    "train_df, test_df = randomize_and_split(main_df, 0.2, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset split X (attributes) + Y (rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['user_id', 'anime_id', 'rating', 'user_encoded', 'anime_encoded']\n"
     ]
    }
   ],
   "source": [
    "print(main_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "814 8 814 152\n",
      "186 8 186 152\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def check_validity(df_collect):\n",
    "    for i in range(len(df_collect)):\n",
    "        if isinstance(df_collect[i], list):\n",
    "            for j in range(len(df_collect[i])):\n",
    "                if df_collect[i][j] is None:\n",
    "                    pass\n",
    "                elif float(df_collect[i][j]) >= 0 and float(df_collect[i][j]) <= 1:\n",
    "                    pass\n",
    "                else:\n",
    "                    raise ValueError(\"check_validity\")\n",
    "\n",
    "def extract_embedding(df, colName, vectorSize):\n",
    "    train = df.select('user_encoded').rdd.flatMap(lambda x: x).collect()\n",
    "    check_validity(train)\n",
    "    # note: since some rows are fully NULL, replaced by a NULL vector\n",
    "    for i in range(len(train)):\n",
    "        if train[i] is None:\n",
    "            train[i] = [None] * vectorSize\n",
    "        elif not isinstance(train[i], list):\n",
    "            train[i] = list(train[i])\n",
    "        if len(train[i]) < vectorSize:\n",
    "            train[i] += [None] * (vectorSize - len(train[i]))\n",
    "\n",
    "    train = np.array(train)\n",
    "\n",
    "    # convert float\n",
    "    nan = np.empty_like(train).flatten()\n",
    "    for i, x in enumerate(train.flat):\n",
    "        try:\n",
    "            nan[i] = float(x)\n",
    "        except:\n",
    "            nan[i] = np.NaN\n",
    "    train = np.array(nan, dtype=np.float32).reshape(train.shape)\n",
    "\n",
    "    return train\n",
    "\n",
    "def prepare_spark_data(df):\n",
    "    # Collect data as separate arrays\n",
    "    # user_data = np.array(df.select(\"user_encoded\").rdd.flatMap(lambda x: x).collect(), dtype=np.int64)\n",
    "    # anime_data = np.array(df.select(\"anime_encoded\").rdd.flatMap(lambda x: x).collect(), dtype=np.int64)\n",
    "    # ratings = np.array(df.select(\"scaled_rating\").rdd.flatMap(lambda x: x).collect(), dtype=np.float32)\n",
    "\n",
    "    ratings = np.array(df.select('rating').rdd.flatMap(lambda x: x).collect(), dtype=np.float32)\n",
    "\n",
    "    # credit: https://stackoverflow.com/a/68907592\n",
    "    user_train = extract_embedding(df, 'user_encoded', len(user_df.columns)-1)\n",
    "    anime_train = extract_embedding(df, 'anime_encoded', len(anime_df.columns)-1)\n",
    "\n",
    "    print(len(user_train), len(user_train[0]), len(anime_train), len(anime_train[0]))\n",
    "    \n",
    "    # return [user_data, anime_data], ratings\n",
    "    return [user_train, anime_train], ratings\n",
    "\n",
    "# Prepare data\n",
    "X_train, y_train = prepare_spark_data(train_df)\n",
    "X_test, y_test = prepare_spark_data(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "722\n",
      "8 152\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_6\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_6\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ user_encoded        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ anime_encoded       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">152</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ user_embedding      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span> │ user_encoded[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ anime_embedding     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">152</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">92,416</span> │ anime_encoded[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dot_product (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dot</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">152</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ user_embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│                     │                   │            │ anime_embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ flatten_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1216</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dot_product[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">77,888</span> │ flatten_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │ dense_12[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ user_encoded        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ anime_encoded       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m152\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ user_embedding      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │        \u001b[38;5;34m768\u001b[0m │ user_encoded[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ anime_embedding     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m152\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │     \u001b[38;5;34m92,416\u001b[0m │ anime_encoded[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dot_product (\u001b[38;5;33mDot\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m152\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ user_embedding[\u001b[38;5;34m0\u001b[0m… │\n",
       "│                     │                   │            │ anime_embedding[\u001b[38;5;34m…\u001b[0m │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ flatten_6 (\u001b[38;5;33mFlatten\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1216\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ dot_product[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_12 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │     \u001b[38;5;34m77,888\u001b[0m │ flatten_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_13 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │         \u001b[38;5;34m65\u001b[0m │ dense_12[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">171,137</span> (668.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m171,137\u001b[0m (668.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">171,137</span> (668.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m171,137\u001b[0m (668.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import keras\n",
    "\n",
    "## Import necessary modules for collaborative filtering\n",
    "from keras.layers import Input, Embedding, Dot, Flatten, Dense\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "def RecommenderNet(num_users, num_animes, embedding_size=128):\n",
    "    # User input layer and embedding layer\n",
    "    user = Input(name='user_encoded', shape=[user_embeddings_size,])\n",
    "    user_embedding = Embedding(name='user_embedding', input_dim=num_users, output_dim=embedding_size)(user)\n",
    "\n",
    "    # Anime input layer and embedding layer\n",
    "    anime = Input(name='anime_encoded', shape=[anime_embeddings_size,])\n",
    "    anime_embedding = Embedding(name='anime_embedding', input_dim=num_animes, output_dim=embedding_size)(anime)\n",
    "\n",
    "    # Dot product of user and anime embeddings\n",
    "    dot_product = Dot(name='dot_product', normalize=True, axes=2)([user_embedding, anime_embedding])\n",
    "    flattened = Flatten()(dot_product)\n",
    "\n",
    "    # Dense layers for prediction\n",
    "    dense = Dense(64, activation='relu')(flattened)\n",
    "    output = Dense(1, activation='sigmoid')(dense)\n",
    "\n",
    "    # Create and compile the model\n",
    "    model = Model(inputs=[user, anime], outputs=output)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=[\"mae\", \"mse\"])\n",
    "\n",
    "    return model\n",
    "\n",
    "print(num_users)\n",
    "print(num_animes)\n",
    "print(user_embeddings_size, anime_embeddings_size)\n",
    "model = RecommenderNet(num_users, num_animes)\n",
    "\n",
    "# Printing my model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model configs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the initial learning rate, minimum learning rate, maximum learning rate, and batch size\n",
    "start_lr = 0.00001\n",
    "min_lr = 0.00001\n",
    "max_lr = 0.00005\n",
    "batch_size = 1000\n",
    "\n",
    "# Define the number of epochs for ramp-up, sustain, and exponential decay\n",
    "rampup_epochs = 5\n",
    "sustain_epochs = 0\n",
    "exp_decay = .8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Configurations setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping\n",
    "\n",
    "def lrfn(epoch):\n",
    "    if epoch < rampup_epochs:\n",
    "        return (max_lr - start_lr) / rampup_epochs * epoch + start_lr\n",
    "    elif epoch < rampup_epochs + sustain_epochs:\n",
    "        return max_lr\n",
    "    else:\n",
    "        return (max_lr - min_lr) * exp_decay**(epoch - rampup_epochs - sustain_epochs) + min_lr\n",
    "\n",
    "# Learning rate scheduler callback\n",
    "lr_callback = LearningRateScheduler(lambda epoch: lrfn(epoch), verbose=0)\n",
    "\n",
    "# File path for saving the model weights\n",
    "checkpoint_filepath = '/model_checkpoint/myanime.weights.h5'\n",
    "\n",
    "# Model checkpoint callback to save the best weights\n",
    "model_checkpoints = ModelCheckpoint(filepath=checkpoint_filepath,\n",
    "                                    save_weights_only=True,\n",
    "                                    monitor='val_loss',\n",
    "                                    mode='min',\n",
    "                                    save_best_only=True)\n",
    "\n",
    "# Early stopping callback to prevent overfitting\n",
    "early_stopping = EarlyStopping(patience=3, monitor='val_loss', mode='min', restore_best_weights=True)\n",
    "\n",
    "# Define the list of callbacks\n",
    "my_callbacks = [\n",
    "    model_checkpoints,\n",
    "    lr_callback,\n",
    "    early_stopping\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Exception encountered when calling Embedding.call().\n\n\u001b[1m{{function_node __wrapped__GatherV2_device_/job:localhost/replica:0/task:0/device:CPU:0}} indices[51,0] = -2147483642 is not in [0, 6) [Op:GatherV2]\u001b[0m\n\nArguments received by Embedding.call():\n  • inputs=tf.Tensor(shape=(814, 8), dtype=float32)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[106], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m      3\u001b[0m tf\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mrun_functions_eagerly(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 5\u001b[0m history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(\n\u001b[1;32m      6\u001b[0m     x\u001b[38;5;241m=\u001b[39mX_train,\n\u001b[1;32m      7\u001b[0m     y\u001b[38;5;241m=\u001b[39my_train,\n\u001b[1;32m      8\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m      9\u001b[0m     epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m,\n\u001b[1;32m     10\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     11\u001b[0m     validation_data\u001b[38;5;241m=\u001b[39m(X_test, y_test),\n\u001b[1;32m     12\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39mmy_callbacks\n\u001b[1;32m     13\u001b[0m )\n\u001b[1;32m     15\u001b[0m model\u001b[38;5;241m.\u001b[39mload_weights(checkpoint_filepath)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Exception encountered when calling Embedding.call().\n\n\u001b[1m{{function_node __wrapped__GatherV2_device_/job:localhost/replica:0/task:0/device:CPU:0}} indices[51,0] = -2147483642 is not in [0, 6) [Op:GatherV2]\u001b[0m\n\nArguments received by Embedding.call():\n  • inputs=tf.Tensor(shape=(814, 8), dtype=float32)"
     ]
    }
   ],
   "source": [
    "# Model training\n",
    "import tensorflow as tf\n",
    "tf.config.run_functions_eagerly(True)\n",
    "\n",
    "history = model.fit(\n",
    "    x=X_train,\n",
    "    y=y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=20,\n",
    "    verbose=1,\n",
    "    validation_data=(X_test, y_test),\n",
    "    callbacks=my_callbacks\n",
    ")\n",
    "\n",
    "model.load_weights(checkpoint_filepath)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
