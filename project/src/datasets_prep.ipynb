{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ALWAYS run Configurations and Related functions before proceeding.\n",
    "\n",
    "Choose 1 out of the 2 options:\n",
    "- Hit \"Run All\" and start from the original datasets (check configs).\n",
    "- Scroll down to Main Dataset section, there is option to load pre-prepped datasets, run the codes starting from there after configurations.\n",
    "\n",
    "For easier readability: search \"VS code jupyter notebook collapse all\".\n",
    "\n",
    "In order to shorten time, use flag show_parts = True.\n",
    "\n",
    "NOTE: In my results, the unified dataset contains 23419469 rows x 155 columns, assuming MAX 4 bytes (float) per data, is around 14.5 GB MAX. So make sure you have at least 15 GB if you intend to output the unified dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/26 13:23:10 WARN Utils: Your hostname, p4stwi2x resolves to a loopback address: 127.0.1.1; using 192.168.100.188 instead (on interface wlp2s0)\n",
      "25/03/26 13:23:10 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/03/26 13:23:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/03/26 13:23:14 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "dataset_input_dir = \"dataset/full/\"\n",
    "dataset_output_dir = \"\"\n",
    "show_parts = False\n",
    "\n",
    "user_dataset_input = dataset_input_dir + \"users-details-2023.csv\"                                                                                                                                                                                                   #p@stwi2x\n",
    "user_dataset_output = dataset_output_dir + \"user_dataset_prep.csv\"\n",
    "anime_dataset_input = dataset_input_dir + \"anime-dataset-2023.csv\"\n",
    "anime_dataset_output = dataset_output_dir + \"anime_dataset_prep.csv\"\n",
    "unified_dataset_input = dataset_input_dir + \"users-score-2023.csv\"\n",
    "unified_dataset_output = dataset_output_dir + \"unified_dataset_prep.csv\"\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession\\\n",
    "    .builder \\\n",
    "    .appName(\"Spark SQL app\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Related functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Simple functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_shape(dataFrame):\n",
    "    print(dataFrame.count(), len(dataFrame.columns))\n",
    "def unique_categories(dataFrame, colName, show: bool = True):\n",
    "    L = dataFrame.select(colName).rdd.flatMap(lambda x: x).collect()\n",
    "    valueSet = {}\n",
    "    for i in L:\n",
    "        if i is not None:\n",
    "            temp = i.split(\", \")\n",
    "            for j in temp:\n",
    "                if colName + \"_\" + j not in valueSet.keys():\n",
    "                    valueSet.update({colName + \"_\" + j: 1})\n",
    "                else:\n",
    "                    valueSet[colName + \"_\" + j] += 1\n",
    "    if show:\n",
    "        print(valueSet)\n",
    "    return valueSet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### One-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cre: https://stackoverflow.com/questions/62408093/one-hot-encoding-multiple-categorical-data-in-a-column\n",
    "import gc\n",
    "def oneHotEncoding(dataFrame, colName: str, show: bool = True, support: int = 0):\n",
    "    size_before = dataFrame.count()\n",
    "\n",
    "    test_df = dataFrame.select(colName)\n",
    "    if show:\n",
    "        print(f\"Fill rate: {test_df.filter(col(colName).isNotNull()).count()}/{test_df.count()}\")\n",
    "\n",
    "    # collect list of unique genres\n",
    "    valueSet = unique_categories(test_df, colName, show=False)\n",
    "    # support\n",
    "    for i in valueSet.copy():\n",
    "        if valueSet[i] < support:\n",
    "            valueSet.pop(i, None)\n",
    "\n",
    "    # transfer unique values to panda DF to map the entries\n",
    "    test_df_pd = test_df.dropDuplicates().toPandas()\n",
    "    test_df_pd = test_df_pd.reindex(test_df_pd.columns.tolist() + list(valueSet), axis=1, fill_value=0)\n",
    "    input_list = test_df_pd[colName].tolist()\n",
    "    for i in range(len(input_list)):\n",
    "        if input_list[i] is not None:\n",
    "            temp = input_list[i].split(\", \")\n",
    "            for j in temp:\n",
    "                if j is not None and colName + \"_\" + j in valueSet.keys():\n",
    "                    test_df_pd.at[i, colName + \"_\" + j] = 1\n",
    "    test_df = spark.createDataFrame(test_df_pd)\n",
    "    if show:\n",
    "        test_df.show()\n",
    "\n",
    "    dataFrame = dataFrame.join(test_df, colName, \"left\").drop(colName) #hmmmm\n",
    "    if show:\n",
    "        dataFrame.show()\n",
    "    # print(f\"Size before: {size_before}\")\n",
    "    # print(f\"Size after: {user_df.count()}\")\n",
    "\n",
    "    # store the embedding column list\n",
    "    embed_column_list = list(valueSet.keys())\n",
    "\n",
    "    # clean up mem\n",
    "    del [[test_df_pd]]\n",
    "    gc.collect()\n",
    "\n",
    "    return dataFrame, embed_column_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Min max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import min, max, col\n",
    "def min_max(dataFrame, colName_in: str, colName_out: str):\n",
    "    temp_df = dataFrame.select(min(colName_in), max(colName_in)).collect()\n",
    "    min_val, max_val = float(temp_df[0][0]), float(temp_df[0][1])\n",
    "\n",
    "    dataFrame = dataFrame.withColumn(colName_out, (col(colName_in)-min_val)/(max_val-min_val))                                                                                                                                                                                         #thien7170\n",
    "\n",
    "    return dataFrame, list([min_val, max_val])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import min, max\n",
    "import pandas as pd\n",
    "\n",
    "def plot(dataFrame, colName: str):\n",
    "    gre_histogram = dataFrame.select(colName).filter(col(colName).cast(\"float\").isNotNull())\\\n",
    "        .rdd.flatMap(lambda x: x)\\\n",
    "        .map(float)\\\n",
    "        .histogram(20)\n",
    "\n",
    "    pd.DataFrame(\n",
    "        list(zip(*gre_histogram)), \n",
    "        columns=['bin', 'frequency']\n",
    "    ).set_index('bin').plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "user_df = spark.read\\\n",
    "    .option(\"header\", True)\\\n",
    "    .option(\"escape\", '\"')\\\n",
    "    .option(\"multiline\", True)\\\n",
    "    .option(\"sep\", \",\")\\\n",
    "    .csv(user_dataset_input)\n",
    "\n",
    "user_embed_columns = {}\n",
    "\n",
    "# element: UNKNOWN\n",
    "user_df = user_df.replace({'UNKNOWN': None, 'Unknown': None})\n",
    "user_df.dropDuplicates()\n",
    "\n",
    "if show_parts:\n",
    "    start_X, start_Y = user_df.count(), len(user_df.columns)\n",
    "    print_shape(user_df)\n",
    "    user_df.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check sparsity for each column\n",
    "from pyspark.sql.functions import col\n",
    "from collections import OrderedDict\n",
    "\n",
    "if show_parts:\n",
    "    rows = user_df.count()\n",
    "    sparsity_score = [user_df.filter(col(Col).isNotNull()).count() / rows for Col in user_df.columns]\n",
    "\n",
    "    sparsity_list = OrderedDict()\n",
    "    for i in range(len(user_df.columns)):\n",
    "        sparsity_list[user_df.columns[i]] = sparsity_score[i]\n",
    "    #spark.createDataFrame([d_list]).show()\n",
    "    print(sparsity_list.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Username"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This column is just user's username as strings, should be dropped to make the dataset lighter and we already have Mal ID as user index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_df = user_df.drop('Username')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gender"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As this categorical column has 2 values at most, we perform one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/26 13:23:30 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "user_df, col_list = oneHotEncoding(user_df, 'Gender', show=show_parts)\n",
    "user_embed_columns['Gender'] = col_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Birthday"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shall convert the non-null values to unix timestamps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split, unix_timestamp\n",
    "split_col = split(user_df['Birthday'], 'T', 2)\n",
    "user_df = user_df.withColumn('Birthday_d', split_col.getItem(0))\\\n",
    "    .withColumn('Birthday_unix', unix_timestamp('Birthday_d', format='yyyy-mm-dd'))\\\n",
    "    .drop('Birthday', 'Birthday_d')\\\n",
    "    .withColumnRenamed('Birthday_unix', 'Birthday')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if show_parts:\n",
    "    user_df.select('Birthday').where('Birthday is not NULL').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is worth noting that anime titles are vastly different for users of different ages, that is why we shall perform user age when giving rating.\n",
    "\n",
    "However, for the time being, we shall perform a simple plotting and eliminating outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "user_df, _ = min_max(user_df, 'Birthday', 'Birthday_m')\n",
    "user_df = user_df.drop('Birthday')\\\n",
    "    .withColumnRenamed('Birthday_m', 'Birthday')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if show_parts:\n",
    "    print_shape(user_df)\n",
    "    plot(user_df, 'Birthday')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "user_df = user_df.filter('Birthday is NULL OR Birthday >= 0.9')\n",
    "user_df, _ = min_max(user_df, 'Birthday', 'Birthday_m')\n",
    "user_df = user_df.drop('Birthday')\\\n",
    "    .withColumnRenamed('Birthday_m', 'Birthday')\n",
    "if show_parts:\n",
    "    print_shape(user_df)\n",
    "    plot(user_df, 'Birthday')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "user_df = user_df.filter('Birthday is NULL OR (Birthday >= 0.4 AND Birthday < 0.5)')\n",
    "user_df, user_embed_columns['Birthday'] = min_max(user_df, 'Birthday', 'Birthday_m')\n",
    "user_df = user_df.drop('Birthday')\\\n",
    "    .withColumnRenamed('Birthday_m', 'Birthday')\n",
    "if show_parts:\n",
    "    print_shape(user_df)\n",
    "    plot(user_df, 'Birthday')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examining the skewness of the observed data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import mean, median\n",
    "if show_parts:\n",
    "    user_df.select(mean('Birthday'), median('Birthday')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the metrics are very close to each other, the skewness of the observed data should not be a concern for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if show_parts:\n",
    "    print_shape(user_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Location (finished)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if show_parts:\n",
    "    user_df.select('Location').where('Location is not NULL').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the location format is very inconsistent, going back and forth between cities and/or countries and imcomprehensible values. \n",
    "\n",
    "It is better for us to drop this column for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_df = user_df.drop('Location')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Joined (finished)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the date when the user joins the MyAnimeList platform. If we intend to use this data to check user's anime-watching duration to give more weights to those who watch more and are able to rate titles more objectively, it shall be better to use the column \"Days Watched\" instead.\n",
    "\n",
    "Hence, this column is dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_df = user_df.drop('Joined')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Days Watched (finished)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This column shows the number of days user has spent watching anime. The more experience one has with watching, one could judge and rate anime titles more accurately.\n",
    "\n",
    "Hence, we use this column, with the name changed for easier SQL queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_df = user_df.withColumnRenamed('Days Watched', 'Days_Watched')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The more time one spends watching anime, the more they can observe and rate anime titles more correctly. And the difference between experienced watchers (such as 1000 and 2000 hours) is not as much as between inexperienced (10 hours) and experienced (500 hours).\n",
    "\n",
    "We shall perform log transformation then standardization and do not perform any outlier elimination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import log10, col\n",
    "user_df = user_df.withColumn('Days_Watched_log', log10(col('Days_Watched')+1.0))\n",
    "user_df = user_df.drop('Days_Watched')\\\n",
    ".withColumnRenamed('Days_Watched_log', 'Days_Watched')\n",
    "\n",
    "user_df, user_embed_columns['Days_Watched'] = min_max(user_df, 'Days_Watched', 'A1')\n",
    "user_df = user_df.drop('Days_Watched').withColumnRenamed('A1', 'Days_Watched')\n",
    "\n",
    "if show_parts:\n",
    "    user_df.select('Days_Watched').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "if show_parts:\n",
    "    print_shape(user_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mean Score (finished)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This column is mean to see how user rates compared to how they usually rate, so it shall be left untouched until joining the datasets into one unified file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Watching, On Hold, Dropped, Plan to Watch, Total Entries (finished)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These columns are to see how the user interacts with anime titles.\n",
    "\n",
    "However, to us, these columns do not mean a lot about user's rating sincerity as we are recommending anime based on tastes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_df = user_df.drop('Watching', 'On Hold', 'Dropped', 'Plan to Watch', 'Total Entries')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Completed, Rewatched, Episodes Watched (finished)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These columns are important in determining user's experience in watching anime, similar to Days Watched. However, since Rewatched only counts number of titles a user has rewatched, without pointing out which titles exactly, and furthermore is similar to, and counted within, the Episodes Watched column.\n",
    "\n",
    "Our decision is to drop Rewatched, and keep the other two, like Days Watched: log transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "user_df = user_df.drop('Rewatched')\\\n",
    "    .withColumnRenamed('Episodes Watched', 'Episodes_Watched')\n",
    "\n",
    "a, b = 'Episodes_Watched', 'Completed'\n",
    "\n",
    "user_df = user_df.withColumn(a + '_l', log10(col(a)+1.0))\\\n",
    "    .withColumn(b+ '_l', log10(col(b)+1.0))\\\n",
    "    .drop(a, b)\\\n",
    "    .withColumnRenamed(a+'_l', a)\\\n",
    "    .withColumnRenamed(b+'_l', b)\n",
    "\n",
    "user_df, user_embed_columns[b] = min_max(user_df, b, b+'_m')\n",
    "user_df, user_embed_columns[a] = min_max(user_df, a, a+'_m')\n",
    "user_df = user_df.drop(a,b)\\\n",
    "    .withColumnRenamed(a+'_m', a)\\\n",
    "    .withColumnRenamed(b+'_m', b)\n",
    "\n",
    "if show_parts:\n",
    "    user_df.select(a,b).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "if show_parts:\n",
    "    print_shape(user_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "if show_parts:\n",
    "    print(\"Before: \" + str(start_X) + \" \" + str(start_Y))\n",
    "    print(\"After: \", end = '')\n",
    "    print_shape(user_df)\n",
    "    user_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "user_df.coalesce(1).write\\\n",
    "        .option(\"header\", \"true\")\\\n",
    "        .option(\"sep\", \",\")\\\n",
    "        .mode(\"overwrite\")\\\n",
    "        .csv(user_dataset_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedded columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('user_embed_col.json', 'w') as fp:\n",
    "    json.dump(user_embed_columns, fp, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anime dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "anime_df = spark.read\\\n",
    "    .option(\"header\", True)\\\n",
    "    .option(\"escape\", '\"')\\\n",
    "    .option(\"multiline\", True)\\\n",
    "    .option(\"sep\", \",\")\\\n",
    "    .csv(anime_dataset_input)\n",
    "\n",
    "anime_embed_columns = {}\n",
    "\n",
    "# element: UNKNOWN\n",
    "anime_df = anime_df.replace({'UNKNOWN': None, 'Unknown': None})\n",
    "anime_df.dropDuplicates()\n",
    "\n",
    "# change column names for better SQL\n",
    "anime_df = anime_df.withColumnRenamed('Scored By', 'Scored_By')\n",
    "\n",
    "if show_parts:\n",
    "    start_X, start_Y = anime_df.count(), len(anime_df.columns)\n",
    "    print_shape(anime_df)\n",
    "    anime_df.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check sparsity for each column\n",
    "from pyspark.sql.functions import col\n",
    "from collections import OrderedDict\n",
    "\n",
    "if show_parts:\n",
    "    rows = anime_df.count()\n",
    "    sparsity_score = [anime_df.filter(col(Col).isNotNull()).count() / rows for Col in anime_df.columns]\n",
    "\n",
    "    sparsity_list = OrderedDict()\n",
    "    for i in range(len(anime_df.columns)):\n",
    "        sparsity_list[anime_df.columns[i]] = sparsity_score[i]\n",
    "    #spark.createDataFrame([d_list]).show()\n",
    "    sparsity_list.items()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Name, English name, Other name (finished)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the problem is about recommendation based on user's taste, it is expected that these features have nothing to do with our recommendation system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "anime_df = anime_df.drop(\"Name\", \"English name\", \"Other name\")\n",
    "if show_parts:\n",
    "    anime_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24905 21\n"
     ]
    }
   ],
   "source": [
    "print_shape(anime_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Score (finished)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As this is a numerical column, we perform data normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "anime_df, anime_embed_columns['Score'] = min_max(anime_df, \"Score\", \"Score_s\")\n",
    "\n",
    "anime_df = anime_df.drop('Score').withColumnRenamed(existing=\"Score_s\",new=\"Score\")\n",
    "\n",
    "if show_parts:\n",
    "    plot(anime_df, \"Score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "if show_parts:\n",
    "    anime_df.select(\"Score\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import median, mode\n",
    "if show_parts:\n",
    "    anime_df.select(median(\"Score\"), mode(\"Score\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though the data is negatively skewed (mean = 0, median = 0.009, mode = 0.171), the skewed-ness is insignificant and can be allowed for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "if show_parts:\n",
    "    print_shape(anime_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Genres (finished)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the column is categorical and the label order in Genres is not important, we use one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "anime_df, col_list = oneHotEncoding(anime_df, \"Genres\", show=show_parts)\n",
    "anime_embed_columns['Genres'] = col_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "if show_parts:\n",
    "    print_shape(anime_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Synopsis (finished)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Synopsis is description of an anime title and has quite some impact to user's taste. However we temporarily leave this out for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "anime_df = anime_df.drop(\"Synopsis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Type (finished)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to Genres, we use one-hot encoding here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "if show_parts:\n",
    "    anime_df.select(\"Type\").groupBy(\"Type\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since only 0.3% of samples have empty values, so we should have no problem dropping these rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|   Type|count|\n",
      "+-------+-----+\n",
      "|     TV| 7597|\n",
      "|Special| 2558|\n",
      "|    OVA| 4076|\n",
      "|  Music| 2686|\n",
      "|  Movie| 4381|\n",
      "|    ONA| 3533|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "anime_df = anime_df.filter(\"Type is not NULL\")\n",
    "anime_df.select(\"Type\").groupBy(\"Type\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we perform one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fill rate: 24831/24831\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+--------+------------+----------+--------+----------+\n",
      "|   Type|Type_TV|Type_OVA|Type_Special|Type_Movie|Type_ONA|Type_Music|\n",
      "+-------+-------+--------+------------+----------+--------+----------+\n",
      "|     TV|      1|       0|           0|         0|       0|         0|\n",
      "|Special|      0|       0|           1|         0|       0|         0|\n",
      "|    OVA|      0|       1|           0|         0|       0|         0|\n",
      "|  Music|      0|       0|           0|         0|       0|         1|\n",
      "|  Movie|      0|       0|           0|         1|       0|         0|\n",
      "|    ONA|      0|       0|           0|         0|       1|         0|\n",
      "+-------+-------+--------+------------+----------+--------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/26 13:24:32 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+--------------------+-----------+----------------+--------------------+--------------------+--------------------+-----------+-------------+--------------------+------+----------+---------+---------+-------+--------------------+------------------+-------------+--------------------+-------------+----------------+------------+--------------+-------------------+--------------+-------------+-------------+--------------+--------------------+---------------+------------+--------------+------------------+-------------+-----------------+----------------+-------------+--------------+-------+--------+------------+----------+--------+----------+\n",
      "|anime_id|Episodes|               Aired|  Premiered|          Status|           Producers|           Licensors|             Studios|     Source|     Duration|              Rating|  Rank|Popularity|Favorites|Scored_By|Members|           Image URL|             Score|Genres_Action|Genres_Award Winning|Genres_Sci-Fi|Genres_Adventure|Genres_Drama|Genres_Mystery|Genres_Supernatural|Genres_Fantasy|Genres_Sports|Genres_Comedy|Genres_Romance|Genres_Slice of Life|Genres_Suspense|Genres_Ecchi|Genres_Gourmet|Genres_Avant Garde|Genres_Horror|Genres_Girls Love|Genres_Boys Love|Genres_Hentai|Genres_Erotica|Type_TV|Type_OVA|Type_Special|Type_Movie|Type_ONA|Type_Music|\n",
      "+--------+--------+--------------------+-----------+----------------+--------------------+--------------------+--------------------+-----------+-------------+--------------------+------+----------+---------+---------+-------+--------------------+------------------+-------------+--------------------+-------------+----------------+------------+--------------+-------------------+--------------+-------------+-------------+--------------+--------------------+---------------+------------+--------------+------------------+-------------+-----------------+----------------+-------------+--------------+-------+--------+------------+----------+--------+----------+\n",
      "|      25|    24.0|Oct 6, 2004 to Ma...|  fall 2004| Finished Airing|GDH, Pony Canyon,...|          Funimation|               Gonzo|      Manga|24 min per ep|R - 17+ (violence...|2167.0|      1546|      822|  53709.0| 134894|https://cdn.myani...| 0.762758620689655|            1|                   0|            1|               1|           0|             0|                  0|             0|            0|            1|             0|                   0|              0|           1|             0|                 0|            0|                0|               0|            0|             0|      1|       0|           0|         0|       0|         0|\n",
      "|      20|   220.0|Oct 3, 2002 to Fe...|  fall 2002| Finished Airing|TV Tokyo, Aniplex...|           VIZ Media|             Pierrot|      Manga|23 min per ep|PG-13 - Teens 13 ...| 599.0|         8|    76343|1883772.0|2717330|https://cdn.myani...| 0.846896551724138|            1|                   0|            0|               1|           0|             0|                  0|             1|            0|            0|             0|                   0|              0|           0|             0|                 0|            0|                0|               0|            0|             0|      1|       0|           0|         0|       0|         0|\n",
      "|      21|    NULL|   Oct 20, 1999 to ?|  fall 1999|Currently Airing|Fuji TV, TAP, Shu...|Funimation, 4Kids...|      Toei Animation|      Manga|       24 min|PG-13 - Teens 13 ...|  55.0|        20|   198986|1226493.0|2168904|https://cdn.myani...|0.9434482758620689|            1|                   0|            0|               1|           0|             0|                  0|             1|            0|            0|             0|                   0|              0|           0|             0|                 0|            0|                0|               0|            0|             0|      1|       0|           0|         0|       0|         0|\n",
      "|       6|    26.0|Apr 1, 1998 to Se...|spring 1998| Finished Airing|Victor Entertainment|Funimation, Geneo...|            Madhouse|      Manga|24 min per ep|PG-13 - Teens 13 ...| 328.0|       246|    15035| 356739.0| 727252|https://cdn.myani...|0.8786206896551726|            1|                   0|            1|               1|           0|             0|                  0|             0|            0|            0|             0|                   0|              0|           0|             0|                 0|            0|                0|               0|            0|             0|      1|       0|           0|         0|       0|         0|\n",
      "|      30|    26.0|Oct 4, 1995 to Ma...|  fall 1995| Finished Airing|TV Tokyo, Kadokaw...|    ADV Films, GKIDS|Gainax, Tatsunoko...|   Original|24 min per ep|PG-13 - Teens 13 ...| 204.0|        45|   100638|1024927.0|1718019|https://cdn.myani...| 0.896551724137931|            1|                   1|            1|               0|           1|             0|                  0|             0|            0|            0|             0|                   0|              1|           0|             0|                 1|            0|                0|               0|            0|             0|      1|       0|           0|         0|       0|         0|\n",
      "|       1|    26.0|Apr 3, 1998 to Ap...|spring 1998| Finished Airing|       Bandai Visual|Funimation, Banda...|             Sunrise|   Original|24 min per ep|R - 17+ (violence...|  41.0|        43|    78525| 914193.0|1771505|https://cdn.myani...|0.9517241379310345|            1|                   1|            1|               0|           0|             0|                  0|             0|            0|            0|             0|                   0|              0|           0|             0|                 0|            0|                0|               0|            0|             0|      1|       0|           0|         0|       0|         0|\n",
      "|      18|    24.0|Apr 17, 2004 to F...|spring 2004| Finished Airing|OB Planning, Stud...|          Funimation|            A.C.G.T.|      Manga|27 min per ep|PG-13 - Teens 13 ...| 393.0|      1273|     1237|  97878.0| 173710|https://cdn.myani...| 0.870344827586207|            1|                   0|            0|               0|           1|             0|                  0|             0|            0|            0|             0|                   0|              0|           0|             0|                 0|            0|                0|               0|            0|             0|      1|       0|           0|         0|       0|         0|\n",
      "|       7|    26.0|Jul 3, 2002 to De...|summer 2002| Finished Airing|Bandai Visual, De...|Funimation, Banda...|             Sunrise|   Original|25 min per ep|PG-13 - Teens 13 ...|2764.0|      1795|      613|  42829.0| 111931|https://cdn.myani...|0.7448275862068966|            1|                   0|            0|               0|           1|             1|                  1|             0|            0|            0|             0|                   0|              0|           0|             0|                 0|            0|                0|               0|            0|             0|      1|       0|           0|         0|       0|         0|\n",
      "|      26|    22.0|Apr 17, 2003 to S...|spring 2003| Finished Airing|Group TAC, Rondo ...|Funimation, Geneo...|            Madhouse|   Original|23 min per ep|    R+ - Mild Nudity| 990.0|       935|     4786|  54857.0| 239423|https://cdn.myani...|0.8151724137931035|            1|                   0|            1|               0|           1|             0|                  0|             0|            0|            0|             0|                   0|              0|           0|             0|                 0|            0|                0|               0|            0|             0|      1|       0|           0|         0|       0|         0|\n",
      "|      29|    26.0|Oct 8, 2004 to Ap...|  fall 2004| Finished Airing|      Marvelous, TBS|Geneon Entertainm...|         Studio Deen|      Manga|24 min per ep|PG-13 - Teens 13 ...|1778.0|      4430|      180|   7273.0|  21391|https://cdn.myani...|0.7765517241379312|            1|                   0|            1|               0|           1|             0|                  0|             0|            0|            0|             0|                   0|              0|           0|             0|                 0|            0|                0|               0|            0|             0|      1|       0|           0|         0|       0|         0|\n",
      "|      23|    12.0|Oct 6, 2004 to De...|  fall 2004| Finished Airing|                NULL|                NULL|      Toei Animation|      Manga|25 min per ep|       PG - Children|6943.0|      8288|        6|   1378.0|   4581|https://cdn.myani...| 0.626206896551724|            1|                   0|            0|               0|           0|             0|                  0|             0|            1|            0|             0|                   0|              0|           0|             0|                 0|            0|                0|               0|            0|             0|      1|       0|           0|         0|       0|         0|\n",
      "|      27|    24.0|Apr 29, 2005 to O...|spring 2005| Finished Airing|                NULL|          Funimation|               Gonzo|Light novel|24 min per ep|R - 17+ (violence...|2591.0|      1233|     1298|  79130.0| 179583|https://cdn.myani...|0.7503448275862068|            1|                   0|            0|               0|           0|             0|                  1|             0|            0|            0|             0|                   0|              0|           0|             0|                 0|            0|                0|               0|            0|             0|      1|       0|           0|         0|       0|         0|\n",
      "|       8|    52.0|Sep 30, 2004 to S...|  fall 2004| Finished Airing|    TV Tokyo, Dentsu|Illumitoon Entert...|      Toei Animation|      Manga|23 min per ep|       PG - Children|4240.0|      5126|       14|   6413.0|  15001|https://cdn.myani...|0.7020689655172414|            0|                   0|            0|               1|           0|             0|                  1|             1|            0|            0|             0|                   0|              0|           0|             0|                 0|            0|                0|               0|            0|             0|      1|       0|           0|         0|       0|         0|\n",
      "|      16|    24.0|Apr 15, 2005 to S...|spring 2005| Finished Airing|Dentsu, Genco, Fu...|VIZ Media, Discot...|           J.C.Staff|      Manga|23 min per ep|PG-13 - Teens 13 ...| 589.0|       862|     4136|  81747.0| 260166|https://cdn.myani...|0.8482758620689655|            0|                   0|            0|               0|           1|             0|                  0|             0|            0|            1|             1|                   0|              0|           0|             0|                 0|            0|                0|               0|            0|             0|      1|       0|           0|         0|       0|         0|\n",
      "|      28|    69.0|Oct 12, 2004 to M...|  fall 2004| Finished Airing|TV Tokyo, Aniplex...|Nozomi Entertainment|             Sunrise|      Manga|24 min per ep|PG-13 - Teens 13 ...| 704.0|      1992|      897|  40492.0|  95380|https://cdn.myani...|0.8358620689655173|            0|                   0|            0|               0|           0|             0|                  0|             0|            0|            1|             0|                   0|              0|           0|             1|                 0|            0|                0|               0|            0|             0|      1|       0|           0|         0|       0|         0|\n",
      "|      24|    26.0|Oct 5, 2004 to Ma...|  fall 2004| Finished Airing|TV Tokyo, Sotsu, ...|          Funimation|        Studio Comet|      Manga|23 min per ep|PG-13 - Teens 13 ...| 756.0|       687|     5139| 146597.0| 320203|https://cdn.myani...|0.8331034482758619|            0|                   0|            0|               0|           0|             0|                  0|             0|            0|            1|             1|                   0|              0|           0|             0|                 0|            0|                0|               0|            0|             0|      1|       0|           0|         0|       0|         0|\n",
      "|      17|    52.0|Sep 11, 2002 to S...|  fall 2002| Finished Airing|                NULL|                NULL|    Nippon Animation|      Manga|23 min per ep|PG-13 - Teens 13 ...|1551.0|      4212|      237|  12960.0|  24172|https://cdn.myani...| 0.786206896551724|            0|                   0|            0|               0|           0|             0|                  0|             0|            1|            1|             0|                   1|              0|           0|             0|                 0|            0|                0|               0|            0|             0|      1|       0|           0|         0|       0|         0|\n",
      "|      19|    74.0|Apr 7, 2004 to Se...|spring 2004| Finished Airing|VAP, Shogakukan-S...|           VIZ Media|            Madhouse|      Manga|24 min per ep|    R+ - Mild Nudity|  26.0|       142|    47235| 368569.0|1013100|https://cdn.myani...|0.9682758620689654|            0|                   0|            0|               0|           1|             1|                  0|             0|            0|            0|             0|                   0|              1|           0|             0|                 0|            0|                0|               0|            0|             0|      1|       0|           0|         0|       0|         0|\n",
      "|      15|   145.0|Apr 6, 2005 to Ma...|spring 2005| Finished Airing|TV Tokyo, Nihon A...|VIZ Media, Sentai...|              Gallop|      Manga|23 min per ep|PG-13 - Teens 13 ...| 688.0|      1252|     1997|  86524.0| 177688|https://cdn.myani...|0.8372413793103448|            0|                   0|            0|               0|           0|             0|                  0|             0|            1|            0|             0|                   0|              0|           0|             0|                 0|            0|                0|               0|            0|             0|      1|       0|           0|         0|       0|         0|\n",
      "|      22|   178.0|Oct 10, 2001 to M...|  fall 2001| Finished Airing|Production I.G, N...|Funimation, VIZ M...|          Trans Arts|      Manga|22 min per ep|PG-13 - Teens 13 ...| 805.0|      1246|     3004|  81992.0| 178273|https://cdn.myani...|0.8289655172413792|            0|                   0|            0|               0|           0|             0|                  0|             0|            1|            0|             0|                   0|              0|           0|             0|                 0|            0|                0|               0|            0|             0|      1|       0|           0|         0|       0|         0|\n",
      "+--------+--------+--------------------+-----------+----------------+--------------------+--------------------+--------------------+-----------+-------------+--------------------+------+----------+---------+---------+-------+--------------------+------------------+-------------+--------------------+-------------+----------------+------------+--------------+-------------------+--------------+-------------+-------------+--------------+--------------------+---------------+------------+--------------+------------------+-------------+-----------------+----------------+-------------+--------------+-------+--------+------------+----------+--------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "anime_df, col_list = oneHotEncoding(anime_df, \"Type\")\n",
    "user_embed_columns['Type'] = col_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24831 45\n"
     ]
    }
   ],
   "source": [
    "print_shape(anime_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Episodes (finished)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anime length (no. of episodes and duration) might have to do with watching trends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "anime_df, anime_embed_columns['Episodes'] = min_max(anime_df, \"Episodes\", \"A1\")\n",
    "anime_df = anime_df.drop('Episodes').withColumnRenamed(\"A1\", \"Episodes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aired (finished)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The format is in the form of \"(start date) to (end date)\", so we perform splitting into two columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# column: Aired \n",
    "from pyspark.sql.functions import split, to_date\n",
    "\n",
    "split_col = split(anime_df[\"Aired\"], \" to \", 2)\n",
    "anime_df = anime_df.withColumn(\"F1\", split_col.getItem(0)).withColumn(\"F2\", split_col.getItem(1))\n",
    "anime_df = anime_df.withColumn(\"air_start\", to_date(col(\"F1\"), \"MMM d, yyyy\"))\\\n",
    "    .withColumn(\"air_end\", to_date(col(\"F2\"), \"MMM d, yyyy\"))\\\n",
    "    .drop(\"F1\", \"F2\", \"Aired\")\n",
    "if show_parts:\n",
    "    anime_df.select(\"air_start\", \"air_end\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some cases, some titles do not have both timestamps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "if show_parts:\n",
    "    print(anime_df.select(\"air_start\", \"air_end\").where(\"air_start is not NULL\").count())\n",
    "    print(anime_df.select(\"air_start\", \"air_end\").where(\"air_end is not NULL\").count())\n",
    "    print(anime_df.select(\"air_start\", \"air_end\").where(\"air_start is not NULL AND air_end is not NULL\").count())                                                               #p4stwi2x\n",
    "    print(anime_df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the number of missing values is huge for both columns and users can manually filter the recommendation rankings, our choice is to drop the columns in processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "anime_df = anime_df.drop(\"air_start\", \"air_end\")\n",
    "if show_parts:\n",
    "    anime_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Premiered (finished)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since some users might have the tendency to watch the latest titles, and so as some for the older titles for nostalgia, trendy eras, etc., we shall use this feature. The feature is in the form of season and year, which we shall transform into real numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split, col\n",
    "\n",
    "if show_parts:\n",
    "    print(f\"Column sparsity: {sparsity_list[\"Premiered\"]}\")\n",
    "\n",
    "split_col = split(anime_df[\"Premiered\"], \" \", 2)\n",
    "anime_df = anime_df.withColumn(\"F1\", split_col.getItem(0)).withColumn(\"F2\", split_col.getItem(1))\n",
    "anime_df = anime_df.na.replace(['spring', 'summer', 'fall', 'winter'], ['0', '0.25', '0.5', '0.75'], 'F1')\\\n",
    "    .withColumn(\"Premiered_f\", col('F1')+col('F2')).drop(\"Premiered\", \"F1\", \"F2\")\n",
    "\n",
    "if show_parts:\n",
    "    plot(anime_df, \"Premiered_f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import mean, median, mode\n",
    "anime_df, anime_embed_columns['Premiered'] = min_max(anime_df, \"Premiered_f\", \"Premiered_fo\")\n",
    "\n",
    "if show_parts:\n",
    "    anime_df.select(median(\"Premiered_fo\"), mode(\"Premiered_fo\")).show()\n",
    "\n",
    "anime_df = anime_df.drop(\"Premiered\", \"Premiered_f\").withColumnRenamed(existing=\"Premiered_fo\", new=\"Premiered\")\n",
    "if show_parts:\n",
    "    plot(anime_df, \"Premiered\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Status (finished)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List of unique values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Status_Finished Airing': 24063,\n",
       " 'Status_Currently Airing': 347,\n",
       " 'Status_Not yet aired': 421}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_categories(anime_df, \"Status\", show=show_parts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the list is short, we shall perform one-hot encoding with no support without any issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "anime_df, col_list = oneHotEncoding(anime_df, \"Status\", show=show_parts)\n",
    "anime_embed_columns['Status'] = col_list\n",
    "if show_parts:\n",
    "    anime_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Producers (finished)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "uVal = unique_categories(anime_df, \"Producers\", show=show_parts)\n",
    "if show_parts:\n",
    "    print(uVal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Too many columns so we have to add support. Firstly we have to plot the frequencies of the categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "if show_parts:\n",
    "    uVal_df = spark.createDataFrame(pd.DataFrame(list(uVal.values()), columns=['col1']))\n",
    "    plot(uVal_df, \"col1\")\n",
    "    plot(uVal_df.where(\"col1 > 60\"), \"col1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "if show_parts:\n",
    "    print(uVal_df.count())\n",
    "    print(uVal_df.where(\"col1 >= 100\").count())\n",
    "    print(uVal_df.where(\"col1 >= 150\").count())\n",
    "    print(uVal_df.where(\"col1 >= 200\").count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we shall get support = 100 for 56 entries, in order to avoid having too much features causing overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "anime_df, col_list = oneHotEncoding(anime_df, \"Producers\", show=show_parts, support=100)\n",
    "anime_embed_columns['Producers'] = col_list\n",
    "if show_parts:\n",
    "    anime_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Licensors (finished)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "if show_parts:\n",
    "    uVal = unique_categories(anime_df, \"Licensors\", show=show_parts)\n",
    "    print(len(uVal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "if show_parts:\n",
    "    uVal_df = spark.createDataFrame(pd.DataFrame(list(uVal.values()), columns=['col1']))\n",
    "    plot(uVal_df, \"col1\")\n",
    "    plot(uVal_df.where(\"col1 < 65\"), \"col1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "if show_parts:\n",
    "    print(uVal_df.count())\n",
    "    print(uVal_df.where(\"col1 >= 30\").count())\n",
    "    print(uVal_df.where(\"col1 >= 50\").count())\n",
    "    print(uVal_df.where(\"col1 >= 100\").count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "anime_df, col_list = oneHotEncoding(anime_df, \"Licensors\", show=show_parts, support=100)\n",
    "anime_embed_columns['Licensors'] = col_list\n",
    "if show_parts:\n",
    "    anime_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Studios (finished)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "if show_parts:\n",
    "    uVal = unique_categories(anime_df, \"Studios\", False)\n",
    "    print(len(uVal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "if show_parts:\n",
    "    uVal_df = spark.createDataFrame(pd.DataFrame(list(uVal.values()), columns=['col1']))\n",
    "    plot(uVal_df, \"col1\")\n",
    "    plot(uVal_df.where(\"col1 > 100\"), \"col1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "if show_parts:\n",
    "    print(uVal_df.count())\n",
    "    print(uVal_df.where(\"col1 >= 100\").count())\n",
    "    print(uVal_df.where(\"col1 >= 150\").count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "anime_df, anime_embed_columns['Studios'] = oneHotEncoding(anime_df, \"Studios\", show=show_parts, support=100)\n",
    "if show_parts:\n",
    "    anime_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Source (finished)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "if show_parts:\n",
    "    uVal = unique_categories(anime_df, \"Source\", False)\n",
    "    print(len(uVal))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of categories is small, so we do not reduce."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "anime_df, col_list = oneHotEncoding(anime_df, \"Source\", show=show_parts, support=100)\n",
    "anime_embed_columns['Source'] = col_list\n",
    "if show_parts:\n",
    "    anime_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Duration (finished)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The format is \"? hr ? min (per ep)\", we shall convert to a single number representing the duration for each episode in minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "temp_df = anime_df.select(\"Duration\")\n",
    "#temp_df.show()\n",
    "\n",
    "data = temp_df.toPandas()[\"Duration\"].tolist()\n",
    "new_d = []\n",
    "for i in range(len(data)):\n",
    "    if data[i] is not None:\n",
    "        S = data[i].split(\" per ep\")[0]\n",
    "        if \"min\" not in S:\n",
    "            if \"hr\" in S:\n",
    "                S += \" 0 min\"\n",
    "            else:\n",
    "                S = \"0 min \" + S\n",
    "        if \"hr\" not in S:\n",
    "            S = \"0 hr \" + S\n",
    "        if \"sec\" not in S:\n",
    "            S += \" 0 sec\"\n",
    "        \n",
    "        h, m, s = S.split(\" hr \")[0], S.split(\" hr \")[1].split(\" min \")[0], S.split(\" hr \")[1].split(\" min \")[1].split(\" sec\")[0]\n",
    "        new_d.append(int(h)*3600+int(m)*60+int(s))\n",
    "    else:\n",
    "        new_d.append(None)\n",
    "if show_parts:\n",
    "    print(len(new_d))\n",
    "data = spark.createDataFrame(pd.DataFrame({\n",
    "    'Duration': data,\n",
    "    'Duration_f': new_d\n",
    "})).dropDuplicates()\n",
    "\n",
    "anime_df = anime_df.join(data, 'Duration', 'left').drop(\"Duration\").withColumnRenamed(existing=\"Duration_f\", new=\"Duration\").dropDuplicates()                   #p4stwi2x\n",
    "if show_parts:\n",
    "    anime_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "if show_parts:\n",
    "    anime_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rating (finished)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This feature shall be used to filter the recommendation list by user's age and/or filter options, hence it is not to be in the processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "anime_df = anime_df.drop(\"Rating\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rank (finished)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the author: \"The rank of the anime based on popularity or other criteria\". Since the metric is not clear and we are here to predict the ranking by ourselves, so it is better to have this feature removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "anime_df = anime_df.drop(\"Rank\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Popularity (finished)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the author: \"The popularity rank of the anime\". It is indeed that the more popular titles are likely to be watched by users, so they should appear higher on our predicted ranking list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "if show_parts:\n",
    "    plot(anime_df, \"Popularity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, it is indeed that this column is just the ranking for the titles, which do not have much value in determining the actual differences between the popularity of two different anime titles. So it is better for us to drop this and come up with a different way to get the popularity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "anime_df = anime_df.drop(\"Popularity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Favorites, Scored By, Members (finished)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we mentioned with the Popularity column, we shall find ways to represent an anime title's popularity. That is why we keep these 3 features, using log normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in ['Favorites', 'Scored_By', 'Members']:\n",
    "    anime_df = anime_df.withColumn(i+'_l', log10(col(i)+1.0))\\\n",
    "        .drop(i).withColumnRenamed(existing=i+'_l', new=i)             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This feature suggests the number of times an anime title is scored. As explained from the Popularity feature, this shall determine how popular a title is to be better on our ranking list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Image URL (finished)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature has nothing to do with data processing, so the column is expected to be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "anime_df = anime_df.drop(\"Image URL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "if show_parts:\n",
    "    print(\"Before: \" + str(start_X) + \" \" + str(start_Y))\n",
    "    print(\"After: \", end = '')\n",
    "    print_shape(anime_df)\n",
    "    anime_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "anime_df.coalesce(1).write\\\n",
    "        .option(\"header\", \"true\")\\\n",
    "        .option(\"sep\", \",\")\\\n",
    "        .mode(\"overwrite\")\\\n",
    "        .csv(anime_dataset_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedded columns list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('anime_embed_col.json', 'w') as fp:\n",
    "    json.dump(anime_embed_columns, fp, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optional: Read pre-prepped side datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: \n",
    "- Ctrl+A to choose all code lines.\n",
    "- Ctrl+K then Ctrl+C to comment all lines selected.\n",
    "- Ctrl+K then Ctrl+U to uncomment all lines selected.\n",
    "- Change the csv paths ONLY."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "anime_df = spark.read\\\n",
    "    .option(\"header\", True)\\\n",
    "    .option(\"escape\", '\"')\\\n",
    "    .option(\"multiline\", True)\\\n",
    "    .option(\"sep\", \",\")\\\n",
    "    .csv(\"anime_dataset_prep.csv\")\n",
    "user_df = spark.read\\\n",
    "    .option(\"header\", True)\\\n",
    "    .option(\"escape\", '\"')\\\n",
    "    .option(\"multiline\", True)\\\n",
    "    .option(\"sep\", \",\")\\\n",
    "    .csv(\"user_dataset_prep.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read main dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_df = spark.read\\\n",
    "    .option(\"header\", True)\\\n",
    "    .option(\"escape\", '\"')\\\n",
    "    .option(\"multiline\", True)\\\n",
    "    .option(\"sep\", \",\")\\\n",
    "    .csv(unified_dataset_input)\n",
    "\n",
    "rating_df.dropDuplicates()\n",
    "\n",
    "if show_parts:\n",
    "    start_X, start_Y = rating_df.count(), len(rating_df.columns)\n",
    "    print_shape(rating_df)\n",
    "    #main_df.show(20) #This takes a while so by default is commented out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Columns: Username, Anime Title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we already have the two ID columns, we shall remove these string columns to make the dataset far lighter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_df = rating_df.drop('Username', 'Anime Title')\n",
    "\n",
    "if show_parts:\n",
    "    print_shape(rating_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Datasets: Join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We perform joining the datasets into one unified dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df = rating_df\\\n",
    "    .join(user_df, rating_df['user_id'] == user_df['Mal ID'])\\\n",
    "    .join(anime_df, rating_df['anime_id'] == anime_df['anime_id'])\n",
    "\n",
    "main_df = main_df.drop(user_df['Mal ID'], anime_df['anime_id'])\n",
    "if show_parts:\n",
    "    print_shape(main_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### User: Mean Score (finished)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned in the user dataset, Mean Score column shall be used with rating score in main dataset to get what users actually gets towards the anime titles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "main_df = main_df.withColumn('actual_rating', col('rating') - col('Mean Score'))\\\n",
    "    .drop('rating', 'Mean Score')\\\n",
    "    .withColumnRenamed('actual_rating', 'rating')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving short version of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: \n",
    "- Ctrl+A to choose all code lines.\n",
    "- Ctrl+K then Ctrl+C to comment all lines selected.\n",
    "- Ctrl+K then Ctrl+U to uncomment all lines selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp_df = main_df.limit(10000)\n",
    "# temp_df.coalesce(1).write\\\n",
    "#         .option(\"header\", \"true\")\\\n",
    "#         .option(\"sep\", \",\")\\\n",
    "#         .mode(\"overwrite\")\\\n",
    "#         .csv(unified_dataset_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving full data (very, very long > 8 mins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: \n",
    "- Ctrl+A to choose all code lines.\n",
    "- Ctrl+K then Ctrl+C to comment all lines selected.\n",
    "- Ctrl+K then Ctrl+U to uncomment all lines selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main_df.coalesce(1).write\\\n",
    "#         .option(\"header\", \"true\")\\\n",
    "#         .option(\"sep\", \",\")\\\n",
    "#         .mode(\"overwrite\")\\\n",
    "#         .csv(unified_dataset_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ending process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "if show_parts:\n",
    "    print_shape(main_df)\n",
    "    main_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
